{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install torchvision\n!pip install pandas_path\n!pip install pytorch-lightning==0.9.*\n!pip install optuna\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-06-05T19:36:55.773706Z","iopub.execute_input":"2023-06-05T19:36:55.774039Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pip\n  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n\u001b[K     |████████████████████████████████| 2.1 MB 4.1 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.1.1\n    Uninstalling pip-20.1.1:\n      Successfully uninstalled pip-20.1.1\nSuccessfully installed pip-23.1.2\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.6.0a0+35d732a)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.18.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.5.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (7.2.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision) (0.18.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport math\nfrom typing import Tuple\nimport seaborn as sn\nimport pandas as pd\nimport pandas_path  # Path style access for pandas\nimport json\nfrom pathlib import Path\nimport logging\nimport random\nimport tarfile\nimport tempfile\nimport warnings\nimport fasttext\n\n# pytorch\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset, WeightedRandomSampler, DataLoader\n\nimport torchvision\nfrom torchvision import transforms, utils\n\nfrom PIL import Image, ImageFilter, ImageEnhance\nfrom tqdm import tqdm\n\n# optuna for hyperparameter optimization\nimport optuna\nfrom optuna.trial import TrialState\nfrom optuna.samplers import TPESampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path.cwd().parent / \"input\" / \"hate-memes\" / \"hateful_memes\"\n\nimg_path = data_dir / \"img\"\ntrain_path = data_dir / \"train.jsonl\"\ndev_path = data_dir / \"dev_seen.jsonl\"\ntest_path = data_dir / \"test_seen.jsonl\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/train.jsonl\",lines=True)\nval = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/dev_seen.jsonl\",lines=True)\ntest = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/test_seen.jsonl\",lines=True)\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"label\"].value_counts().plot(kind=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Blancing train set - to prevent biased model and poor performance.\nWe'll use oversampling for that.","metadata":{}},{"cell_type":"code","source":"class_weights = [1.0,5481/3019]\nsample_weights = [0] * len(train)\n\nfor idx in range(len(train)) :\n    class_weight = class_weights[train.label[idx]]\n    sample_weights[idx] = class_weight\n    \n# replacement=True - we want to see the same sample\n# more than once during training.\nsampler = WeightedRandomSampler(sample_weights, \n                                num_samples=len(sample_weights),\n                                replacement=True)\n# for later - after transform\n# train_loader = DataLoader(data, batch_size=batch_size, sampler=sampler)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nimg = plt.imread(f\"/kaggle/input/hate-memes/hateful_memes/img/42953.png\")\nplt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [\n    Image.open(f\"/kaggle/input/hate-memes/hateful_memes/{train.img[i]}\").convert(\"RGB\")\n    for i in range(5)\n]\n\nfor image in images:\n    print(image.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a callable image_transform with Compose\nimage_transform = transforms.Compose(\n    [\n        # we'll need to resize the images to form tensor \n        # minibatches appropriate for training a model.\n        transforms.Resize(size=(224, 224)),\n        transforms.ToTensor()\n    ]\n)\n\n# convert the images and prepare for visualization.\ntensor_img = torch.stack(\n    [image_transform(image) for image in images]\n)\ngrid = utils.make_grid(tensor_img)\n\n# plot\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.axis('off')\n_ = plt.imshow(grid.permute(1, 2, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HatefulMemesDataset(torch.utils.data.Dataset):\n    \"\"\"Uses jsonl data to preprocess and serve \n    dictionary of multimodal tensors for model input.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        img_dir,\n        image_transform,\n        text_transform,\n        balance=False,\n        dev_limit=None,\n        random_state=0,\n    ):\n\n        self.samples_frame = pd.read_json(\n            data_path, lines=True\n        )\n        self.dev_limit = dev_limit\n        if balance:\n            neg = self.samples_frame[\n                self.samples_frame.label.eq(0)\n            ]\n            pos = self.samples_frame[\n                self.samples_frame.label.eq(1)\n            ]\n            self.samples_frame = pd.concat(\n                [\n                    neg.sample(\n                        pos.shape[0], \n                        random_state=random_state\n                    ), \n                    pos\n                ]\n            )\n        if self.dev_limit:\n            if self.samples_frame.shape[0] > self.dev_limit:\n                self.samples_frame = self.samples_frame.sample(\n                    dev_limit, random_state=random_state\n                )\n        self.samples_frame = self.samples_frame.reset_index(\n            drop=True\n        )\n        self.samples_frame.img = self.samples_frame.apply(\n            lambda row: (img_dir / row.img), axis=1\n        )\n\n        # https://github.com/drivendataorg/pandas-path\n        if not self.samples_frame.img.apply(lambda x: (img_dir / x).exists()).all():\n            raise FileNotFoundError\n        if not (self.samples_frame.img.apply(lambda x: (img_dir / x).is_file())).all():\n            raise TypeError\n            \n        self.image_transform = image_transform\n        self.text_transform = text_transform\n\n    def __len__(self):\n        \"\"\"This method is called when you do len(instance) \n        for an instance of this class.\n        \"\"\"\n        return len(self.samples_frame)\n\n    def __getitem__(self, idx):\n        \"\"\"This method is called when you do instance[key] \n        for an instance of this class.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_id = self.samples_frame.loc[idx, \"id\"]\n\n        image = Image.open(\n            self.samples_frame.loc[idx, \"img\"]\n        ).convert(\"RGB\")\n        image = self.image_transform(image)\n\n        text = torch.Tensor(\n            self.text_transform.get_sentence_vector(\n                self.samples_frame.loc[idx, \"text\"]\n            )\n        ).squeeze()\n\n        if \"label\" in self.samples_frame.columns:\n            label = torch.Tensor(\n                [self.samples_frame.loc[idx, \"label\"]]\n            ).long().squeeze()\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text, \n                \"label\": label\n            }\n        else:\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text\n            }\n\n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LanguageAndVisionConcat(torch.nn.Module):\n    def __init__(\n        self,\n        num_classes,\n        loss_fn,\n        language_module,\n        vision_module,\n        language_feature_dim,\n        vision_feature_dim,\n        fusion_output_size,\n        dropout_p,\n        \n    ):\n        super(LanguageAndVisionConcat, self).__init__()\n        self.language_module = language_module\n        self.vision_module = vision_module\n        self.fusion = torch.nn.Linear(\n            in_features=(language_feature_dim + vision_feature_dim), \n            out_features=fusion_output_size\n        )\n        self.fc = torch.nn.Linear(\n            in_features=fusion_output_size, \n            out_features=num_classes\n        )\n        self.loss_fn = loss_fn\n        self.dropout = torch.nn.Dropout(dropout_p)\n        \n    def forward(self, text, image, label=None):\n        text_features = torch.nn.functional.relu(\n            self.language_module(text)\n        )\n        image_features = torch.nn.functional.relu(\n            self.vision_module(image)\n        )\n        combined = torch.cat(\n            [text_features, image_features], dim=1\n        )\n        fused = self.dropout(\n            torch.nn.functional.relu(\n            self.fusion(combined)\n            )\n        )\n        logits = self.fc(fused)\n        pred = torch.nn.functional.softmax(logits)\n        loss = (\n            self.loss_fn(pred, label) \n            if label is not None else label\n        )\n        return (pred, loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\n\n# for the purposes of this post, we'll filter\n# much of the lovely logging info from our LightningModule\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger().setLevel(logging.WARNING)\n\n\nclass HatefulMemesModel(pl.LightningModule):\n    def __init__(self, hparams):\n        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n            # ok, there's one for-loop but it doesn't count\n            if data_key not in hparams.keys():\n                raise KeyError(\n                    f\"{data_key} is a required hparam in this model\"\n                )\n        \n        super(HatefulMemesModel, self).__init__()\n        self.hparams = hparams\n        \n        # assign some hparams that get used in multiple places\n        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n        self.language_feature_dim = self.hparams.get(\n            \"language_feature_dim\", 300\n        )\n        self.vision_feature_dim = self.hparams.get(\n            # balance language and vision features by default\n            \"vision_feature_dim\", self.language_feature_dim\n        )\n        self.output_path = Path(\n            self.hparams.get(\"output_path\", \"model-outputs\")\n        )\n        self.output_path.mkdir(exist_ok=True)\n        \n        # instantiate transforms, datasets\n        self.text_transform = self._build_text_transform()\n        self.image_transform = self._build_image_transform()\n        self.train_dataset = self._build_dataset(\"train_path\")\n        self.dev_dataset = self._build_dataset(\"dev_path\")\n        \n        # set up model and training\n        self.model = self._build_model()\n        self.trainer_params = self._get_trainer_params()\n    \n    ## Required LightningModule Methods (when validating) ##\n    \n    def forward(self, text, image, label=None):\n        return self.model(text, image, label)\n\n    def training_step(self, batch, batch_nb):\n        # Use mixed precision training to leverage hardware \n        # acceleration through mixed precision computations.\n        preds, loss = self.forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        \n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_nb):\n        preds, loss = self.eval().forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        \n        return {\"batch_val_loss\": loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack(\n            tuple(\n                output[\"batch_val_loss\"] \n                for output in outputs\n            )\n        ).mean()\n        \n        return {\n            \"val_loss\": avg_loss,\n            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n        }\n\n    def configure_optimizers(self):\n        optimizers = [\n            torch.optim.AdamW(\n                self.model.parameters(), \n                lr=self.hparams.get(\"lr\", 0.001)\n            )\n        ]\n        schedulers = [\n            torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizers[0]\n            )\n        ]\n        return optimizers, schedulers\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset, \n            shuffle=True, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dev_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n    \n    ## Convenience Methods ##\n    \n    def fit(self):\n        self._set_seed(self.hparams.get(\"random_state\", 42))\n        self.trainer = pl.Trainer(**self.trainer_params)\n        self.trainer.fit(self)\n        \n        \n    def _set_seed(self, seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n\n    def _build_text_transform(self):\n        # using FastText tokenizer\n        with tempfile.NamedTemporaryFile() as ft_training_data:\n            ft_path = Path(ft_training_data.name)\n            with ft_path.open(\"w\") as ft:\n                training_data = [\n                    json.loads(line)[\"text\"] + \"/n\" \n                    for line in open(\n                        self.hparams.get(\"train_path\")\n                    ).read().splitlines()\n                ]\n                for line in training_data:\n                    ft.write(line + \"\\n\")\n                language_transform = fasttext.train_unsupervised(\n                    str(ft_path),\n                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n                    dim=self.embedding_dim\n                )\n        return language_transform\n\n    \n    \"\"\" # to use BERT tokenizer:\n        text_transform = text.transforms.Compose(\n            [\n                text.transforms.BertTokenizer(\n                    pretrained_model_name=\"bert-base-uncased\",\n                    max_seq_length=20, # from our EDA\n                    do_lower_case=True,\n                ),\n                text.transforms.BertNumericEncoding(\n                    pretrained_model_name=\"bert-base-uncased\",\n                    max_seq_length=20, # from our EDA\n                    do_lower_case=True,\n                )\n            ]\n        )\n        return text_transform\n    \"\"\"\n\n        \n        \n    \n    def _build_image_transform(self):\n        image_dim = self.hparams.get(\"image_dim\", 224)\n        image_transform = torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize(\n                    size=(image_dim, image_dim)\n                ),        \n                torchvision.transforms.ToTensor(),\n                # all torchvision models expect the same\n                # normalization mean and std\n                # https://pytorch.org/docs/stable/torchvision/models.html\n                torchvision.transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406), \n                    std=(0.229, 0.224, 0.225)\n                ),\n            ]\n        )\n        return image_transform\n\n    def _build_dataset(self, dataset_key):\n        return HatefulMemesDataset(\n            data_path=self.hparams.get(dataset_key, dataset_key),\n            img_dir=self.hparams.get(\"img_dir\"),\n            image_transform=self.image_transform,\n            text_transform=self.text_transform,\n            # limit training samples only\n            dev_limit=(\n                self.hparams.get(\"dev_limit\", None) \n                if \"train\" in str(dataset_key) else None\n            ),\n            balance=True if \"train\" in str(dataset_key) else False,\n        )\n    \n    def _build_model(self):\n        # we're going to pass the outputs of our text\n        # transform through an additional trainable layer\n        # rather than fine-tuning the transform\n        language_module = torch.nn.Linear(\n                in_features=self.embedding_dim,\n                out_features=self.language_feature_dim\n        )\n        \n        # easiest way to get features rather than\n        # classification is to overwrite last layer\n        # with an identity transformation, we'll reduce\n        # dimension using a Linear layer, resnet is 2048 out\n        vision_module = torchvision.models.resnet152(\n            pretrained=True\n        )\n        vision_module.fc = torch.nn.Linear(\n                in_features=2048,\n                out_features=self.vision_feature_dim\n        )\n\n        return LanguageAndVisionConcat(\n            num_classes=self.hparams.get(\"num_classes\", 2),\n            loss_fn=torch.nn.CrossEntropyLoss(),\n            language_module=language_module,\n            vision_module=vision_module,\n            language_feature_dim=self.language_feature_dim,\n            vision_feature_dim=self.vision_feature_dim,\n            fusion_output_size=self.hparams.get(\n                \"fusion_output_size\", 512\n            ),\n            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n        )\n    \n    def _get_trainer_params(self):\n        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n            filepath=self.output_path,\n            monitor=self.hparams.get(\n                \"checkpoint_monitor\", \"avg_val_loss\"\n            ),\n            mode=self.hparams.get(\n                \"checkpoint_monitor_mode\", \"min\"\n            ),\n            verbose=self.hparams.get(\"verbose\", True)\n        )\n\n        early_stop_callback = pl.callbacks.EarlyStopping(\n            monitor=self.hparams.get(\n                \"early_stop_monitor\", \"avg_val_loss\"\n            ),\n            min_delta=self.hparams.get(\n                \"early_stop_min_delta\", 0.001\n            ),\n            patience=self.hparams.get(\n                \"early_stop_patience\", 3\n            ),\n            verbose=self.hparams.get(\"verbose\", True),\n        )\n\n        trainer_params = {\n            \"checkpoint_callback\": checkpoint_callback,\n            \"early_stop_callback\": early_stop_callback,\n            \"default_root_dir\": self.output_path,\n            \"accumulate_grad_batches\": self.hparams.get(\n                \"accumulate_grad_batches\", 1\n            ),\n            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n            \"gradient_clip_val\": self.hparams.get(\n                \"gradient_clip_value\", 1\n            ),\n        }\n        return trainer_params\n            \n    @torch.no_grad()\n    def make_submission_frame(self, test_path):\n        test_dataset = self._build_dataset(test_path)\n        submission_frame = pd.DataFrame(\n            index=test_dataset.samples_frame.id,\n            columns=[\"proba\", \"label\"]\n        )\n        test_dataloader = torch.utils.data.DataLoader(\n            test_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16))\n        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n            preds, _ = self.model.eval().to(\"cpu\")(\n                batch[\"text\"], batch[\"image\"]\n            )\n            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n        submission_frame.proba = submission_frame.proba.astype(float)\n        submission_frame.label = submission_frame.label.astype(int)\n        return submission_frame\n    \n    \n\"\"\"\nthe architecture of the model:\n    # 1. text_transform: FastText num of parameters: 0 because we use the pretrained model\n    2. image_transform: ResNet152 num of parameters: 58,279,234\n    3. language_module: Linear(in_features=300, out_features=512, bias=True) num of parameters: 153,600\n    4. vision_module: Linear(in_features=2048, out_features=512, bias=True) num of parameters: 1,049,088\n    5. fusion: Linear(in_features=1024, out_features=512, bias=True) num of parameters: 524,800\n    6. output: Linear(in_features=512, out_features=2, bias=True) num of parameters: 1,026\n    total num of parameters: 59,007,748\n\"\"\"\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams = {\n    \n    # Required hparams\n    \"train_path\": train_path,\n    \"dev_path\": dev_path,\n    \"img_dir\": data_dir,\n    \n    # Optional hparams\n    \"embedding_dim\": 150,\n    \"language_feature_dim\": 300,\n    \"vision_feature_dim\": 300,\n    \"fusion_output_size\": 256,\n    \"output_path\": \"model-outputs\",\n    \"dev_limit\": None,\n    \"lr\": 0.00005,\n    \"max_epochs\": 4,\n    \"n_gpu\": 1, # torch.cuda.device_count(),\n    \"batch_size\": 4,\n    # allows us to \"simulate\" having larger batches \n    \"accumulate_grad_batches\": 16, \n    \"early_stop_patience\": 3,\n}\n\n#hateful_memes_model = HatefulMemesModel(hparams=hparams)\n#hateful_memes_model.(fit)()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# use optuna to tune hyperparameters\n# define objective function\ndef objective(trial):\n    # sample hpsearch params\n    hparams[\"lr\"] = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n    hparams[\"dropout_p\"] = trial.suggest_float(\"dropout_p\", 0.2, 0.5)\n    hparams[\"accumulate_grad_batches\"] = trial.suggest_categorical(\n        \"accumulate_grad_batches\", [1, 2, 4, 8, 16, 32]\n    )\n    hparams[\"batch_size\"] = trial.suggest_categorical(\n        \"batch_size\", [4, 8, 16, 32, 64]\n    )\n    hparams[\"language_feature_dim\"] = trial.suggest_int(\n        \"language_feature_dim\", 100, 500, step=10\n    )\n    hparams[\"vision_feature_dim\"] = trial.suggest_int(\n        \"vision_feature_dim\", 100, 500, step=10\n    )\n    hparams[\"fusion_output_size\"] = trial.suggest_int(\n        \"fusion_output_size\", 100, 500, step=10\n    )\n\n    \n    # train model\n    hateful_memes_model = HatefulMemesModel(hparams=hparams)\n    hateful_memes_model.fit()\n    \n    # report results back to optuna\n    return hateful_memes_model.trainer.callback_metrics[\"val_loss\"].item()\n\n\n# define optuna params\nstudy = optuna.create_study(direction=\"minimize\")\n\n# optimize model\nstudy.optimize(objective, n_trials=100, timeout=3600, show_progress_bar=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we should only have saved the best checkpoint\ncheckpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\nassert len(checkpoints) == 1\n\ncheckpoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\n    checkpoints[0].as_posix()\n)\nsubmission = hateful_memes_model.make_submission_frame(\n    test_path\n)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.groupby(\"label\").proba.mean()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.label.value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv((\"model-outputs/submission.csv\"), index=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\n\n# Assuming you have the submission frame with \"proba\" and \"label\" columns\nproba = torch.tensor(submission['proba'].values)\nlabel = torch.tensor(test['label'].values)\n\n# Calculate AUC-ROC score\nauc_roc = roc_auc_score(label, proba)\nprint(f\"AUC-ROC Score: {auc_roc}\")\n\n# Calculate accuracy\npredictions = proba.round().long()\naccuracy = accuracy_score(label, predictions)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OCR (Optical Character Recognition) ","metadata":{}},{"cell_type":"code","source":"!pip install pytesseract","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytesseract\n\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\nimage_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n\n# Convert the image to grayscale\nimage = Image.open(image_path).convert(\"L\")\n\nimage = image.filter(ImageFilter.MedianFilter())\n\ntext = pytesseract.image_to_string(image, lang='eng')\n\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade easyocr\n!pip install --upgrade opencv-python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import easyocr\nimport cv2\n\nreader = easyocr.Reader(['en'])\n\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\nimage_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n\nresult = reader.readtext(image_path)\n\nextracted_text = []\nfor detection in result:\n    text = detection[1]\n    # Perform post-processing to handle \"o\" and \"0\" confusion\n    #text = text.replace('0', 'o')\n    extracted_text.append(text)\n    \nfor text in extracted_text:\n    print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}