{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aviv360/memesense?scriptVersionId=135511283\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade torch==1.7\n!pip install pandas_path\n!pip install optuna\n!pip install pymysql\n!pip install --upgrade wandb\n!pip install --upgrade transformers","metadata":{"_uuid":"7214a6b2-727b-422a-8e15-8e2a202b1fd1","_cell_guid":"d7a735d2-4448-4787-bf8b-c9c44966fcaa","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"_uuid":"31180ba0-8163-4b3f-bdca-ee7c3aa79e6a","_cell_guid":"81daaeed-f9d5-40ac-a034-9f1f5fa88479","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.132502Z","iopub.status.idle":"2023-07-02T14:13:13.136003Z","shell.execute_reply.started":"2023-07-02T14:13:13.135611Z","shell.execute_reply":"2023-07-02T14:13:13.135657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport sys\nimport os\nimport math\nfrom typing import Tuple\nimport seaborn as sn\nimport pandas as pd\nimport pandas_path  # Path style access for pandas\nimport json\nfrom pathlib import Path\nimport logging\nimport random\nimport tarfile\nimport tempfile\nimport warnings\n\nfrom transformers import  CLIPVisionModel, CLIPVisionConfig, AutoTokenizer, CLIPTokenizer\nimport transformers\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n# pytorch\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset, WeightedRandomSampler, DataLoader\nfrom torch.optim.lr_scheduler import StepLR, ExponentialLR, ReduceLROnPlateau\nfrom torch.cuda.amp import autocast, GradScaler\n\n# from efficientnet_pytorch import EfficientNet\nfrom ignite.contrib.handlers import create_lr_scheduler_with_warmup\n\nimport torchvision\nfrom torchvision import transforms, utils\n\nfrom PIL import Image, ImageFilter, ImageEnhance\nfrom tqdm import tqdm\n\n# optuna for hyperparameter optimization\nimport optuna\nfrom optuna.trial import TrialState\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour, plot_intermediate_values, plot_optimization_history, plot_parallel_coordinate, plot_slice\n\nimport kornia\nfrom kornia import augmentation as K\nfrom kornia.augmentation import AugmentationSequential\n\n\nimport torch\nimport clip\nfrom PIL import Image\n\nimport pymysql\npymysql.install_as_MySQLdb()\n\n\nfrom IPython.display import clear_output\n\ndevice0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice1 = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"_uuid":"8c899783-a99b-4248-8e86-203975df0837","_cell_guid":"24095a78-9ed5-4867-ad3b-72a904052cd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.141882Z","iopub.status.idle":"2023-07-02T14:13:13.145308Z","shell.execute_reply.started":"2023-07-02T14:13:13.144932Z","shell.execute_reply":"2023-07-02T14:13:13.144966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getTokenizer():\n    return CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") #AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndef gettextmodel(configuration):\n    text_model = clip.load(\"ViT-B/32\", device=device0)\n    return text_model","metadata":{"_uuid":"49335971-d41e-4067-9af2-d089e9663b23","_cell_guid":"1df4f455-afb3-4cd9-966a-e4d6aa92c44f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.149656Z","iopub.status.idle":"2023-07-02T14:13:13.150781Z","shell.execute_reply.started":"2023-07-02T14:13:13.150434Z","shell.execute_reply":"2023-07-02T14:13:13.150478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"def getTokenizer():\n    return clip.tokenize\n\ndef gettextmodel(configuration):\n    text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n    return text_model\"\"\"","metadata":{"_uuid":"02ab675c-c551-4440-ab99-463cb0f6021a","_cell_guid":"0fc706eb-fb35-486c-86d3-3c148a86c0d7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.158416Z","iopub.status.idle":"2023-07-02T14:13:13.159548Z","shell.execute_reply.started":"2023-07-02T14:13:13.159185Z","shell.execute_reply":"2023-07-02T14:13:13.159231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path.cwd().parent / \"input\" / \"hate-memes\" / \"hateful_memes\"\n\nimg_path = data_dir / \"img\"\ntrain_path = data_dir / \"train.jsonl\"\ndev_path = data_dir / \"dev_seen.jsonl\"\ntest_path = data_dir / \"test_seen.jsonl\"","metadata":{"_uuid":"434fc05e-503b-4354-8ea3-5c8c2fc0dac7","_cell_guid":"04602f5a-dd59-4e8d-8ea9-2866c59dd4b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.161579Z","iopub.status.idle":"2023-07-02T14:13:13.162697Z","shell.execute_reply.started":"2023-07-02T14:13:13.162343Z","shell.execute_reply":"2023-07-02T14:13:13.162376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/train.jsonl\",lines=True)\nval = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/dev_seen.jsonl\",lines=True)\ntest = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/test_seen.jsonl\",lines=True)","metadata":{"_uuid":"1b219087-0925-41ae-876e-3be3089d7c49","_cell_guid":"108a2d6f-7782-4434-b473-95c77758ef64","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.164727Z","iopub.status.idle":"2023-07-02T14:13:13.171932Z","shell.execute_reply.started":"2023-07-02T14:13:13.171584Z","shell.execute_reply":"2023-07-02T14:13:13.171617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"b3276c2f-9f38-49c5-99a5-b55fa727a11b","_cell_guid":"626c36b9-ceb7-4ec5-b6be-d8d08eeeae89","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.174018Z","iopub.status.idle":"2023-07-02T14:13:13.175137Z","shell.execute_reply.started":"2023-07-02T14:13:13.17477Z","shell.execute_reply":"2023-07-02T14:13:13.174802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"_uuid":"f30461a1-ea74-421b-a121-080fbbf5e89c","_cell_guid":"6b4344d1-d99b-4bdb-ba5e-8a763353e480","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.177311Z","iopub.status.idle":"2023-07-02T14:13:13.178478Z","shell.execute_reply.started":"2023-07-02T14:13:13.178088Z","shell.execute_reply":"2023-07-02T14:13:13.178121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"label\"].value_counts().plot(kind=\"bar\")","metadata":{"_uuid":"3367bdcf-16a0-40d0-8877-874b09433baa","_cell_guid":"c121d0e5-07db-4d41-b547-14843995ad61","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.180503Z","iopub.status.idle":"2023-07-02T14:13:13.187931Z","shell.execute_reply.started":"2023-07-02T14:13:13.187586Z","shell.execute_reply":"2023-07-02T14:13:13.187619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Blancing train set - to prevent biased model and poor performance.\nWe'll use oversampling for that.","metadata":{"_uuid":"472f64d5-7733-47b3-9ae5-301d6349543e","_cell_guid":"8137d055-f6d7-46e1-a4ef-e266fe24fd5d","trusted":true}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nimg = plt.imread(f\"/kaggle/input/hate-memes/hateful_memes/img/42953.png\")\nplt.imshow(img)","metadata":{"_uuid":"e9ea4e80-0d12-4532-a108-9afe8c31c307","_cell_guid":"62787f7b-629a-4018-848a-c9df1d328bdb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.189866Z","iopub.status.idle":"2023-07-02T14:13:13.190949Z","shell.execute_reply.started":"2023-07-02T14:13:13.190607Z","shell.execute_reply":"2023-07-02T14:13:13.190639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [\n    Image.open(f\"/kaggle/input/hate-memes/hateful_memes/{train.img[i]}\").convert(\"RGB\")\n    for i in range(5)\n]\n\nfor image in images:\n    print(image.size)","metadata":{"_uuid":"d4d0954e-e544-4273-b9db-29bb68790a17","_cell_guid":"1372918f-de33-474e-b72d-cd78130c1502","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.192948Z","iopub.status.idle":"2023-07-02T14:13:13.194104Z","shell.execute_reply.started":"2023-07-02T14:13:13.193765Z","shell.execute_reply":"2023-07-02T14:13:13.193797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhparams = {\n    \n    # Required hparams\n    \"train_path\": train_path,\n    \"dev_path\": dev_path,\n    \"img_dir\": data_dir,\n    \n    # Optional hparams\n    \"image_dim\": 224,\n    \"text_feature_dim\": 512,\n    \"vision_feature_dim\": 512,\n    \"fusion_mid_size\": 512,\n    \"fusion_output_size\": 1024,\n    \"output_path\": \"model-outputs\",\n    \"dev_limit\": None,\n    \"lr\": 5e-5,\n    \"max_epochs\": 10,\n    \"batch_size\": 64,\n    \"early_stop_patience\": 41,\n    \"bert_dropout_p\": 0.2,\n    \"bert_attn_dropout_p\": 0.2,\n    \"dropout_fusion_mid_p\": 0.1,\n    \"dropout_fusion_out_p\": 0.05,\n    \"dropout_vision_feature_p\": 0.05,\n    \"dropout_text_feature_p\": 0.05,\n\n    # Image augmentations params\n    \"horizontal_flip_p\": 0.5,        \n    \"rotation\": 3,\n    \"brightness\": 0.02,\n    \"contrast\": 0.02,\n    \"saturation\": 0.02,\n    \"hue\": 0.02,\n    \"random_erasing_p\": 0.5,\n    \"random_erasing_scale_min\": 0.02,\n    \"random_erasing_scale_max\": 0.033,\n    \"random_erasing_ratio_min\": 0.3,\n    \"random_erasing_ratio_max\": 3.3,\n    \"random_erasing_value\": 0,\n    \"gaussian_noise_std\": 0.1,\n    \"enable_augmentation\": True,\n    \n    \n}","metadata":{"_uuid":"92686b3c-9e8a-4251-bb75-5dab2c2dcaca","_cell_guid":"6fcce4e8-6f62-4bf4-8df3-dee0f86578e1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.196114Z","iopub.status.idle":"2023-07-02T14:13:13.197263Z","shell.execute_reply.started":"2023-07-02T14:13:13.196899Z","shell.execute_reply":"2023-07-02T14:13:13.196932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# define a callable image_transform with Compose\ndef imgtrans(hparams):\n    if (hparams[\"enable_augmentation\"] == False):\n        return AugmentationSequential(\n            K.Resize(size=(hparams[\"image_dim\"],hparams[\"image_dim\"])),\n            K.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]),\n                                          std=torch.tensor([0.229, 0.224, 0.225])),\n        )\n    else:\n        return AugmentationSequential(\n            K.Resize(size=(hparams[\"image_dim\"],hparams[\"image_dim\"])),\n            K.RandomHorizontalFlip(p=hparams[\"horizontal_flip_p\"]),\n            K.RandomRotation(degrees=hparams[\"rotation\"]),\n            K.ColorJitter(brightness=hparams[\"brightness\"],\n                                            contrast=hparams[\"contrast\"],\n                                            saturation=hparams[\"saturation\"],\n                                            hue=hparams[\"hue\"]),\n            K.RandomErasing(p=hparams[\"random_erasing_p\"],\n                                            scale=(hparams[\"random_erasing_scale_min\"],\n                                                    hparams[\"random_erasing_scale_max\"]),\n                                            ratio=(hparams[\"random_erasing_ratio_min\"],\n                                                    hparams[\"random_erasing_ratio_max\"]),\n                                            value=hparams[\"random_erasing_value\"]),\n            K.Normalize(mean=torch.tensor([0.485, 0.456, 0.406]),\n                                          std=torch.tensor([0.229, 0.224, 0.225])),\n            K.RandomGaussianNoise(mean=0., std=hparams[\"gaussian_noise_std\"]),\n            K.Resize(size=(hparams[\"image_dim\"], hparams[\"image_dim\"])),\n            K.RandomCrop(size=(hparams[\"image_dim\"], hparams[\"image_dim\"]),\n                                            padding=None,\n                                            pad_if_needed=False,\n                                            fill=0,\n                                            padding_mode='constant'),\n            same_on_batch=False\n        )\nPILtoTesor = transforms.ToTensor()\nimage_transform = imgtrans(hparams)\n\n# convert the images and prepare for visualization.\ntensor_img = torch.stack(\n    [image_transform(PILtoTesor(image)).squeeze() for image in images]\n)\ngrid = utils.make_grid(tensor_img)\n\n# plot\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.axis('off')\n_ = plt.imshow(grid.permute(1, 2, 0))","metadata":{"_uuid":"50a649f1-13ab-4543-a645-e704c04675a6","_cell_guid":"df6f63e8-c308-4134-b6d1-fbe93aeefee6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.206891Z","iopub.status.idle":"2023-07-02T14:13:13.207332Z","shell.execute_reply.started":"2023-07-02T14:13:13.207104Z","shell.execute_reply":"2023-07-02T14:13:13.207124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HatefulMemesDataset(torch.utils.data.Dataset):\n    \"\"\"Uses jsonl data to preprocess and serve \n    dictionary of multimodal tensors for model input.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        img_dir,\n        image_transform,\n        text_transform,\n        balance=False,\n        dev_limit=None,\n        random_state=0,\n    ):\n\n        self.samples_frame = pd.read_json(\n            data_path, lines=True\n        )\n        # Reset the index of the DataFrame (for sampler use)\n        # self.samples_frame.reset_index(drop=True, inplace=True)\n\n        self.dev_limit = dev_limit\n        if balance:\n            neg = self.samples_frame[\n                self.samples_frame.label.eq(0)\n            ]\n            pos = self.samples_frame[\n                self.samples_frame.label.eq(1)\n            ]\n            '''self.samples_frame = pd.concat(\n                [\n                    neg,\n                    pos.sample(\n                        neg.shape[0], \n                        random_state=random_state,\n                        replace=True\n                    ) \n                ]\n            )'''\n            self.samples_frame = pd.concat(\n                [\n                        \n                    neg.sample(\n                        pos.shape[0], \n                        random_state=random_state\n                    ), \n                    pos\n                ]\n            )\n            \n        if self.dev_limit:\n            if self.samples_frame.shape[0] > self.dev_limit:\n                self.samples_frame = self.samples_frame.sample(\n                    dev_limit, random_state=random_state\n                )\n        self.samples_frame = self.samples_frame.reset_index(\n            drop=True\n        )\n        self.samples_frame.img = self.samples_frame.apply(\n            lambda row: (img_dir / row.img), axis=1\n        )\n\n        # https://github.com/drivendataorg/pandas-path\n        if not self.samples_frame.img.apply(lambda x: (img_dir / x).exists()).all():\n            raise FileNotFoundError\n        if not (self.samples_frame.img.apply(lambda x: (img_dir / x).is_file())).all():\n            raise TypeError\n            \n        self.image_transform = image_transform\n        self.text_transform = text_transform\n\n    def __len__(self):\n        \"\"\"This method is called when you do len(instance) \n        for an instance of this class.\n        \"\"\"\n        return len(self.samples_frame)\n\n    def __getitem__(self, idx):\n        \"\"\"This method is called when you do instance[key] \n        for an instance of this class.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_id = self.samples_frame.loc[idx, \"id\"]\n\n        \n        \n        image = Image.open(\n            self.samples_frame.loc[idx, \"img\"]\n        ).convert(\"RGB\")\n        image = self.image_transform(PILtoTesor(image)).squeeze()\n\n        text = torch.Tensor(\n            self.text_transform.encode_plus(\n                self.samples_frame.loc[idx, \"text\"],\n                add_special_tokens=True,\n                max_length=77,\n                pad_to_max_length=True,\n                truncation=True,\n            )[\"input_ids\"]\n        ).long().squeeze()\n\n        if \"label\" in self.samples_frame.columns:\n            label = torch.Tensor(\n                [self.samples_frame.loc[idx, \"label\"]]\n            ).long().squeeze()\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text, \n                \"label\": label\n            }\n        else:\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text\n            }\n\n        return sample","metadata":{"_uuid":"f27b14aa-306a-44d7-aa85-8d2a93417c9b","_cell_guid":"9b330ca4-be94-4ebe-8279-b4c8051fa088","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.208841Z","iopub.status.idle":"2023-07-02T14:13:13.209485Z","shell.execute_reply.started":"2023-07-02T14:13:13.209236Z","shell.execute_reply":"2023-07-02T14:13:13.209256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class textAndVisionConcat(torch.nn.Module):\n    def __init__(\n        self,\n        num_classes,\n        loss_fn,\n        text_module,\n        vision_module,\n        text_feature_dim,\n        vision_feature_dim,\n        fusion_mid_size,\n        fusion_output_size,\n        dropout_vision_feature_p,\n        dropout_fusion_mid_p,\n        dropout_fusion_out_p,\n        dropout_text_feature_p\n    ):\n        super(textAndVisionConcat, self).__init__()\n\n        self.text_module = text_module\n        self.text_feature_dropout = torch.nn.Dropout(dropout_text_feature_p)\n        self.vision_module = vision_module\n        self.vision_feature_dropout = torch.nn.Dropout(dropout_vision_feature_p)\n\n        self.fusion = torch.nn.Sequential(\n            torch.nn.Linear(\n                in_features=text_feature_dim+vision_feature_dim,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_fusion_mid_p),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_output_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_fusion_out_p)\n        )\n\n\n        self.fc = torch.nn.Linear(\n            in_features=fusion_output_size, \n            out_features=num_classes\n        )\n        \n        self.loss_fn = loss_fn\n        \n\n        # initialize weights\n        # initialize weights of fusion layers\n        for layer in self.fusion:\n            if isinstance(layer, torch.nn.Linear):\n                torch.nn.init.xavier_uniform_(layer.weight)\n                torch.nn.init.zeros_(layer.bias)\n        # initialize weights of fc layer\n        torch.nn.init.xavier_uniform_(self.fc.weight)\n        torch.nn.init.zeros_(self.fc.bias)\n        \n    def forward(self, text, image, label=None):\n        text_features = torch.nn.functional.relu(\n            self.text_module(text)\n        )\n\n        image_features = torch.nn.functional.relu(\n            self.vision_module(image)\n        )\n\n        combined = torch.cat(\n            [text_features, image_features], dim=1\n        )\n        combined = combined.to(torch.float32)\n        \n        # Attention mechanism\n        '''attention_weights = torch.softmax(torch.mul(text_features, image_features), dim=1)\n        weighted_text_features = torch.mul(text_features, attention_weights)\n        weighted_image_features = torch.mul(image_features, attention_weights)\n        \n        combined = torch.cat(\n            [weighted_text_features, weighted_image_features], dim=1\n        )''' \n        \n        \n        fused = self.fusion(combined)\n        #fused = self.fusion(combined)\n        \n        logits = self.fc(fused)\n        pred = torch.nn.functional.softmax(logits, dim=1)\n        loss = (\n            self.loss_fn(pred, label) \n            if label is not None else label\n        )\n        return (pred, loss)","metadata":{"_uuid":"9f8b53c1-35d5-4554-9b5d-1068464fde7d","_cell_guid":"32889c35-2b05-4dcb-9ee2-05b45e1f63bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.211921Z","iopub.status.idle":"2023-07-02T14:13:13.212496Z","shell.execute_reply.started":"2023-07-02T14:13:13.212254Z","shell.execute_reply":"2023-07-02T14:13:13.212276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HatefulMemesModel(torch.nn.Module):\n    def __init__(self, hparams):\n        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n            # ok, there's one for-loop but it doesn't count\n            if data_key not in hparams.keys():\n                raise KeyError(\n                    f\"{data_key} is a required hparam in this model\"\n                )\n        \n        super(HatefulMemesModel, self).__init__()\n        self.hparams = hparams\n        \n        # assign some hparams that get used in multiple places\n        self.text_feature_dim = self.hparams.get(\n            \"text_feature_dim\", 300\n        )\n        self.vision_feature_dim = self.hparams.get(\n            # balance text and vision features by default\n            \"vision_feature_dim\", self.text_feature_dim\n        )\n        self.output_path = Path(\n            self.hparams.get(\"output_path\", \"model-outputs\")\n        )\n        self.output_path.mkdir(exist_ok=True)\n        \n        # instantiate transforms, datasets\n        self.text_transform = self._build_text_transform()\n        self.image_transform = self._build_image_transform()\n        self.train_dataset = self._build_dataset(\"train_path\")\n        print(f\"Train size: {len(self.train_dataset)}\")\n        self.dev_dataset = self._build_dataset(\"dev_path\")\n        print(f\"val size: {len(self.dev_dataset)}\")\n        \n        # set up model and training\n        self.model = self._build_model().to(device0) \n        self.optimizer, self.scheduler = self.configure_optimizers() \n        \n    def forward(self, text, image, label=None):\n        return self.model(text, image, label)\n\n    def training_step(self, batch, batch_nb):\n        preds, loss = self.forward(\n            text=batch[\"text\"].to(device0), \n            image=batch[\"image\"].to(device0), \n            label=batch[\"label\"].to(device0)\n        )\n        \n        return preds, loss\n\n    def validation_step(self, batch, batch_nb):\n        preds, loss = self.eval().forward(\n            text=batch[\"text\"].to(device0), \n            image=batch[\"image\"].to(device0), \n            label=batch[\"label\"].to(device0)\n        )\n        \n        return preds, loss\n\n    # TODO - clac validation error here...\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack(\n            tuple(\n                output[\"batch_val_loss\"] \n                for output in outputs\n            )\n        ).mean()\n        \n        return {\n            \"val_loss\": avg_loss,\n            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n        }\n\n    def configure_optimizers(self):\n        # AdamW with L2 regularization.\n        optimizer = torch.optim.AdamW(self.model.parameters(), \n                                      lr=self.hparams.get(\"lr\", 0.001))\n\n    \n        \n        '''optimizer = torch.optim.SGD(self.model.parameters(), lr=self.hparams.get(\"lr\", 0.001), \n                                    momentum=0.9, nesterov=True)'''\n\n      \n        \n        # scheduler = ExponentialLR(optimizer, gamma=0.8)\n        \n        # patience = 0, after 1 bad epoch, reduce LR\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=3, verbose=True)\n        \n        # scheduler = StepLR(optimizer, step_size=2, gamma=0.5)\n\n        # warmup_duration - number of epochs for which the warm-up will be performed.\n        # doesnt seem to help raising the lr - causes overfitting.\n        '''scheduler_with_warmup = create_lr_scheduler_with_warmup(scheduler,\n                                           warmup_start_value=1e-5,\n                                           warmup_duration=2,\n                                           warmup_end_value=1e-4)'''\n        \n        return optimizer, scheduler\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset,\n            shuffle=True,\n            batch_size=self.hparams.get(\"batch_size\", 64), \n            num_workers=self.hparams.get(\"num_workers\", 2),\n            pin_memory=True\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dev_dataset, \n            shuffle=False, \n            batch_size=int(math.ceil(len(self.dev_dataset)/4)), \n            num_workers=self.hparams.get(\"num_workers\", 2)\n        )\n    \n    def fit(self):\n        self._set_seed(self.hparams.get(\"random_state\", 42))\n        self.epochs = self.hparams.get(\"max_epochs\", 10)\n        self.train_loader = self.train_dataloader()\n        self.val_loader = self.val_dataloader()\n        # self.best_val_loss = float('inf')\n        self.best_error_rate = 1.0\n        self.epochs_without_improvement = 0\n        \n        lrs = [] \n                        \n        for epoch in range(1, self.epochs + 1):\n            \n            lrs.append(self.optimizer.param_groups[0]['lr'])\n            \n            # Updates lr (for warmup)\n            # self.scheduler(None) \n            \n            sys.stderr.flush()\n            \n            print('Epoch {}, lr {}'.format(\n                epoch, self.optimizer.param_groups[0]['lr']), flush=True)\n            \n            self.train_epoch(epoch) \n            is_early_stopping_happened = self.val_epoch()[1]\n            \n            if is_early_stopping_happened:\n                break\n                \n        plt.plot(range(1, len(lrs) + 1), lrs)\n        plt.title(\"Learning Rate Schedule\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Learning Rate\")\n        # Set the tick locations and labels for integer values\n        plt.xticks(range(1, len(lrs) + 1, 1))\n        plt.show()\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        \n        train_loss = 0\n        num_batches = len(self.train_loader)\n        \n        scaler = GradScaler(enabled=True)\n                \n        # Initialize tqdm progress bar\n        pbar = tqdm(total=num_batches, \n                    desc=f\"Epoch {epoch}\", \n                    unit=\"batch\",\n                    position=0, \n                    leave=True)\n                \n        for batch_idx, batch in enumerate(self.train_loader):\n                \n            # Runs the forward pass with autocasting.\n            #with autocast(enabled=True):\n            preds, loss = self.training_step(batch, batch_idx)\n                \n            self.optimizer.zero_grad(set_to_none=True)\n                            \n\n            loss.backward()\n            # Update model parameters.\n            \n            self.optimizer.step()\n            \n            \n            train_loss += loss.item()\n            \n            # Update tqdm progress bar\n            pbar.set_postfix({\"Train Loss\":  f\"{(train_loss / (batch_idx + 1)):.4f}\"})\n            pbar.update()\n            \n        pbar.close() \n        sys.stderr.flush()\n                    \n        train_loss /= len(self.train_loader)\n        \n        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}\", flush=True) \n        \n        \n    \n    @torch.no_grad()\n    def val_epoch(self):\n        self.model.eval()\n        \n        # val_loss = 0\n        num_errors = 0\n        error_rate = 0.0\n        num_batches = len(self.val_loader)\n        \n        error_rates = []\n        \n        # Initialize tqdm progress bar\n        '''pbar = tqdm(total=num_batches, \n                    desc=f\"Validation\", \n                    unit=\"batch\",\n                    position=0, \n                    leave=True)'''\n        \n        pbar = tqdm(total=num_batches, \n                    desc=f\"Validation Error Rate\", \n                    unit=\"batch\",\n                    position=0, \n                    leave=True)\n                \n        for batch_idx, batch in enumerate(self.val_loader):\n            preds, loss = self.validation_step(batch, batch_idx)\n            \n            # val_loss += loss.item()\n            \n            predicted_labels = preds.argmax(dim=1).to('cpu')\n            \n            actual_labels = batch['label'].to('cpu')\n\n            # Calculate the number of misclassified samples (zero - one loss)\n            error_rate = (predicted_labels != actual_labels).float().mean().item()\n            \n            error_rates.append(error_rate)\n            \n            # Update tqdm progress bar\n            # pbar.set_postfix({\"Validation Loss\": val_loss / (batch_idx + 1)})\n            pbar.set_postfix({\"Validation Error Rate\": f\"{error_rate:.4f}\"})\n            pbar.update()\n        \n        pbar.close()\n        sys.stderr.flush()\n        \n        # val_loss /= len(self.val_loader)\n        \n        mean_error_rate = np.array(error_rates).mean()\n\n        # print(f\"Validation Loss: {val_loss:.4f}\", flush=True)\n        print(f\"Validation Error Rate: {mean_error_rate:.4f}\", flush=True)\n        \n        # Updates lr (without warmup)\n        self.scheduler.step(mean_error_rate)\n\n        #if val_loss < self.best_val_loss:\n        if mean_error_rate < self.best_error_rate:\n            # self.best_val_loss = val_loss\n            self.best_error_rate = mean_error_rate\n            self.save_model()\n            self.epochs_without_improvement = 0\n\n        else:\n            self.epochs_without_improvement += 1\n\n            if self.epochs_without_improvement >= self.hparams.get(\"early_stop_patience\", 5):\n                print(\"Training stopped due to early stopping.\")\n                sys.stderr.flush()\n                return (mean_error_rate,True)\n        \n        return (mean_error_rate, False)\n    \n    def save_model(self):\n        output_path = os.path.join(self.hparams.get(\"output_path\"), \"best_model.ckpt\")\n        torch.save(self.model.state_dict(), output_path)\n        \n    def _set_seed(self, seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n\n    def _build_text_transform(self):\n            # using the tokenizer \n            text_transform = getTokenizer()\n            return text_transform\n\n        \n    def _build_image_transform(self):\n        return imgtrans(self.hparams)\n        \n    def _build_dataset(self, dataset_key):\n        return HatefulMemesDataset(\n            data_path=self.hparams.get(dataset_key, dataset_key),\n            img_dir=self.hparams.get(\"img_dir\"),\n            image_transform=self.image_transform,\n            text_transform=self.text_transform,\n            # limit training samples only\n            dev_limit=(\n                self.hparams.get(\"dev_limit\", None) \n                if \"train\" in str(dataset_key) else None\n            ),\n            # blance = True for train causes undersampling\n            # of train data.\n            balance=True if \"train\" in str(dataset_key) else False,\n        )\n        \n    def _build_model(self):\n        model, preprocess = clip.load(\"ViT-B/32\", device=device0)\n\n        # load pretrained text model\n        text_module = model.encode_text\n        vision_module = model.encode_image\n        \n        \n      \n        \n        return textAndVisionConcat(\n            num_classes=self.hparams.get(\"num_classes\", 2),\n            loss_fn=torch.nn.CrossEntropyLoss(),\n            text_module=text_module,\n            vision_module=vision_module,\n            text_feature_dim=self.text_feature_dim,\n            vision_feature_dim=self.vision_feature_dim,\n            fusion_mid_size=self.hparams.get(\"fusion_mid_size\", 1024),\n            fusion_output_size=self.hparams.get(\n                \"fusion_output_size\", 512\n            ),\n            dropout_fusion_mid_p=self.hparams.get(\"dropout_fusion_mid_p\"),\n            dropout_fusion_out_p=self.hparams.get(\"dropout_fusion_out_p\"),\n            dropout_vision_feature_p=self.hparams.get(\"dropout_vision_feature_p\"),\n            dropout_text_feature_p=self.hparams.get(\"dropout_text_feature_p\"),\n        )\n\n    @torch.no_grad()\n    def make_submission_frame(self, test_path):\n        test_dataset = self._build_dataset(test_path)\n        submission_frame = pd.DataFrame(\n            index=test_dataset.samples_frame.id,\n            columns=[\"proba\", \"label\"]\n        )\n        test_dataloader = torch.utils.data.DataLoader(\n            test_dataset, \n            shuffle=False, \n            batch_size=int(math.ceil(len(test_dataset)/4)), \n            num_workers=self.hparams.get(\"num_workers\", 2)\n        )\n\n        self.model.eval()\n        for batch in tqdm(test_dataloader, total=len(test_dataloader), position=0, leave=True):\n            preds, _ = self.model.to(device0)(\n                batch[\"text\"].to(device0), batch[\"image\"].to(device0)\n            )\n            \n            preds = preds.to(\"cpu\")\n            \n            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n        submission_frame.proba = submission_frame.proba.astype(float)\n        submission_frame.label = submission_frame.label.astype(int)\n        return submission_frame","metadata":{"_uuid":"386b7517-4017-41e0-8a07-054a0a0f325c","_cell_guid":"4fccf333-dcf2-4835-93f9-957c6ed54074","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.214271Z","iopub.status.idle":"2023-07-02T14:13:13.214803Z","shell.execute_reply.started":"2023-07-02T14:13:13.214567Z","shell.execute_reply":"2023-07-02T14:13:13.214588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model = HatefulMemesModel(hparams=hparams)","metadata":{"_uuid":"6bb2e23e-6b14-4d57-a459-3852ec4404b2","_cell_guid":"8f2f18f4-ee7b-4a65-80fd-b033455d2f71","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.220234Z","iopub.status.idle":"2023-07-02T14:13:13.220775Z","shell.execute_reply.started":"2023-07-02T14:13:13.220542Z","shell.execute_reply":"2023-07-02T14:13:13.220563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model.fit()","metadata":{"_uuid":"22b551c1-d39e-44b9-b0e9-b4c27bf428f6","_cell_guid":"beeeb023-17b0-4448-b871-6f03c81b48d4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.222436Z","iopub.status.idle":"2023-07-02T14:13:13.223046Z","shell.execute_reply.started":"2023-07-02T14:13:13.222812Z","shell.execute_reply":"2023-07-02T14:13:13.222833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nthe architecture of the model:\n    # 1. text_transform: FastText num of parameters: 0 because we use the pretrained model without fine-tuning\n    2. image_transform: ResNet152 num of parameters: 58,279,234\n    3. text_module: Linear(in_features=300, out_features=512, bias=True) num of parameters: 153,600\n    4. vision_module: Linear(in_features=2048, out_features=512, bias=True) num of parameters: 1,049,088\n    5. fusion: Linear(in_features=1024, out_features=512, bias=True) num of parameters: 524,800\n    6. output: Linear(in_features=512, out_features=2, bias=True) num of parameters: 1,026\n    total num of parameters: 59,007,748\n\"\"\"","metadata":{"_uuid":"8e2e19cd-7d65-403c-9998-3deff0f00d2d","_cell_guid":"07cabe06-5f2f-448f-b7b4-a449ecbf4177","trusted":true}},{"cell_type":"code","source":"\n#study = optuna.create_study(direction=\"minimize\",study_name=\"test2\",storage = uri2)\n\n# define study\nstudy = optuna.load_study(study_name=\"test2\", storage=uri2) # uri2 = your DB uri","metadata":{"_uuid":"77531cbe-dd85-40d9-876e-29c5b206a598","_cell_guid":"a6d88117-cf5a-4a40-8a9a-1efd77102a50","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.224912Z","iopub.status.idle":"2023-07-02T14:13:13.225423Z","shell.execute_reply.started":"2023-07-02T14:13:13.225197Z","shell.execute_reply":"2023-07-02T14:13:13.225218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams[\"max_epochs\"] = 6\nhparams[\"dev_limit\"] = None\nhparams[\"batch_size\"] = 64","metadata":{"_uuid":"cbd0e8bd-661a-4b46-a745-7f6a73803dd2","_cell_guid":"7858b3b5-c118-4a26-96bd-619b82f7e526","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.226949Z","iopub.status.idle":"2023-07-02T14:13:13.227499Z","shell.execute_reply.started":"2023-07-02T14:13:13.227258Z","shell.execute_reply":"2023-07-02T14:13:13.227279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use optuna to tune hyperparameters\n# define objective function\ndef objective(trial):\n    # sample hpsearch params\n\n    hparams[\"lr\"] = trial.suggest_float(\"lr\",1e-5, 1e-3, log=True)\n    #hparams[\"dropout_in_p\"] = trial.suggest_float(\"dropout_in_p\", 0, 0.5, step=0.01)\n    #hparams[\"dropout_mid_p\"] = trial.suggest_float(\"dropout_mid_p\", 0, 0.5, step=0.01)\n    #hparams[\"dropout_out_p\"] = trial.suggest_float(\"dropout_out_p\", 0, 0.5, step=0.01)\n    #hparams[\"bert_dropout_p\"] = trial.suggest_float(\"bert_dropout_p\", 0, 0.5, step=0.01)\n    #hparams[\"bert_attn_dropout_p\"] = trial.suggest_float(\"bert_attn_dropout_p\", 0, 0.5, step=0.01)\n    hparams[\"dropout_fusion_mid_p\"] = trial.suggest_float(\"dropout_fusion_mid_p\", 0, 0.5, step=0.01)\n    hparams[\"dropout_fusion_out_p\"] = trial.suggest_float(\"dropout_fusion_out_p\", 0, 0.5, step=0.01)\n    hparams[\"dropout_vision_feature_p\"] = trial.suggest_float(\"dropout_vision_feature_p\", 0, 0.5, step=0.01)\n    hparams[\"dropout_text_feature_p\"] = trial.suggest_float(\"dropout_text_feature_p\", 0, 0.5, step=0.01)\n    \n    hparams[\"horizontal_flip_p\"] = trial.suggest_float(\"horizontal_flip_p\", 0.0, 1.0)\n    hparams[\"rotation\"] = trial.suggest_int(\"rotation\", 0, 15)\n    hparams[\"brightness\"] = trial.suggest_float(\"brightness\", 0.0, 0.3)\n    hparams[\"contrast\"] = trial.suggest_float(\"contrast\", 0.0, 0.3)\n    hparams[\"saturation\"] = trial.suggest_float(\"saturation\", 0.0, 0.3)\n    hparams[\"hue\"] = trial.suggest_float(\"hue\", 0.0, 0.3)\n    hparams[\"random_erasing_p\"] = trial.suggest_float(\"random_erasing_p\", 0.0, 1.0)\n    hparams[\"random_erasing_scale_min\"] = trial.suggest_float(\"random_erasing_scale_min\", 0.0, 0.1)\n    hparams[\"random_erasing_scale_max\"] = trial.suggest_float(\"random_erasing_scale_max\", hparams[\"random_erasing_scale_min\"], 0.5)\n    hparams[\"random_erasing_ratio_min\"] = trial.suggest_float(\"random_erasing_ratio_min\", 0.1, 1.0)\n    hparams[\"random_erasing_ratio_max\"] = trial.suggest_float(\"random_erasing_ratio_max\", 1.0, 10.0)\n    hparams[\"random_erasing_value\"] = trial.suggest_float(\"random_erasing_value\", 0.0, 0.6)\n    hparams[\"gaussian_noise_std\"] = trial.suggest_float(\"gaussian_noise_std\", 0.0, 1.1)\n\n    \n    # train model\n    model = HatefulMemesModel(hparams=hparams)\n    model._set_seed(model.hparams.get(\"random_state\", 42))\n    model.epochs = model.hparams.get(\"max_epochs\", 10)\n    model.train_loader = model.train_dataloader()\n    model.val_loader = model.val_dataloader()\n    # model.best_val_loss = float('inf')\n    model.best_error_rate = 1.0\n    model.epochs_without_improvement = 0\n\n    \n    best_error_rate = 1\n    for epoch in range(1, model.epochs + 1):\n        \n        \n        #clear_output(wait=False)\n        \n        model.train_epoch(epoch)\n        \n        #validation:\n        with torch.no_grad():\n\n            model.model.eval()\n\n            num_errors = 0\n            error_rate = 0.0\n            num_batches = len(model.val_loader)\n\n            error_rates = torch.Tensor()\n\n            # Initialize tqdm progress bar        \n            #pbar = tqdm(total=num_batches, \n            #            desc=f\"Validation Error Rate\", \n            #            unit=\"batch\",\n            #            position=0, \n            #            leave=True)\n\n            for batch_idx, batch in enumerate(model.val_loader):\n                preds, loss = model.validation_step(batch, batch_idx)\n\n                # val_loss += loss.item()\n\n                predicted_labels = preds.argmax(dim=1).to('cpu',torch.float32)\n\n                actual_labels = batch['label'].to('cpu',torch.float32)\n\n                # Calculate the number of misclassified samples (zero - one loss)\n                error_rate = (predicted_labels != actual_labels)#.mean().item()\n\n                error_rates = torch.cat([error_rates,(predicted_labels != actual_labels)])#, dim=0)\n\n                # Update tqdm progress bar\n                # pbar.set_postfix({\"Validation Loss\": val_loss / (batch_idx + 1)})\n                #pbar.set_postfix({\"Validation Error Rate\": f\"{error_rate:.4f}\"})\n                #pbar.update()\n\n            #pbar.close()\n            sys.stderr.flush()\n\n            # val_loss /= len(model.val_loader)\n\n            mean_error_rate = torch.mean(error_rates)\n            best_error_rate = (mean_error_rate if mean_error_rate < best_error_rate else best_error_rate)\n            # print(f\"Validation Loss: {val_loss:.4f}\", flush=True)\n            print(f\"Validation Error Rate: {mean_error_rate:.4f}\", flush=True)\n\n            # Report validation loss value back to Optuna\n            trial.report(mean_error_rate, epoch)\n\n            # Handle pruning based on the loss value.\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n     \n    return mean_error_rate\n\n# run optuna\nstudy.optimize(objective, n_trials=20, timeout=36000, show_progress_bar=True)","metadata":{"_uuid":"20d1a1d8-eab9-488a-80b2-70f9865bfd84","_cell_guid":"e14a9c2b-8ab9-4881-a55c-f00c47d2789f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.229253Z","iopub.status.idle":"2023-07-02T14:13:13.232327Z","shell.execute_reply.started":"2023-07-02T14:13:13.232093Z","shell.execute_reply":"2023-07-02T14:13:13.232115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear gpu cache\ntorch.cuda.empty_cache()","metadata":{"_uuid":"2fc4f3e1-afcb-44db-a312-0b9199ee5f50","_cell_guid":"cc9bbe93-455d-49dc-92d7-0be4e49e78ef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.233759Z","iopub.status.idle":"2023-07-02T14:13:13.234191Z","shell.execute_reply.started":"2023-07-02T14:13:13.233967Z","shell.execute_reply":"2023-07-02T14:13:13.233988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hparams.update(study.trials[579].params)","metadata":{"_uuid":"86b144c2-7629-4c96-a0f8-cfe016ed8384","_cell_guid":"858750dd-56a0-4e58-bef4-a101bb2d8154","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.236177Z","iopub.status.idle":"2023-07-02T14:13:13.236751Z","shell.execute_reply.started":"2023-07-02T14:13:13.236521Z","shell.execute_reply":"2023-07-02T14:13:13.236543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams.update(study.best_trial.params)\n#study.best_trial.params","metadata":{"_uuid":"fa706290-ad69-4946-9a86-949b8d93eaac","_cell_guid":"e91c7626-f61c-46f7-8a33-4ff4f8c3cfcb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.240368Z","iopub.status.idle":"2023-07-02T14:13:13.241982Z","shell.execute_reply.started":"2023-07-02T14:13:13.241747Z","shell.execute_reply":"2023-07-02T14:13:13.241769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams[\"dev_limit\"] = None\nhparams[\"early_stop_patience\"] = 4\nhparams[\"max_epochs\"] = 40\nhparams[\"lr\"] = 1e-4","metadata":{"_uuid":"ab374ee5-1720-4624-ae6b-482390e06928","_cell_guid":"e41d1ed3-638d-4449-aa72-8980ea7c280f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.243855Z","iopub.status.idle":"2023-07-02T14:13:13.250804Z","shell.execute_reply.started":"2023-07-02T14:13:13.250548Z","shell.execute_reply":"2023-07-02T14:13:13.25057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model = HatefulMemesModel(hparams=hparams)","metadata":{"_uuid":"2d203c3b-b1cb-4f50-8aea-46fffb989a63","_cell_guid":"535aae68-53ef-4861-837d-769041691f1b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.252133Z","iopub.status.idle":"2023-07-02T14:13:13.252671Z","shell.execute_reply.started":"2023-07-02T14:13:13.25243Z","shell.execute_reply":"2023-07-02T14:13:13.25245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model.fit()","metadata":{"_uuid":"6ac8ab15-1eeb-4c21-a47e-7bff407ef46e","_cell_guid":"9d6c3913-2395-4c22-97af-f27fd3129069","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.254293Z","iopub.status.idle":"2023-07-02T14:13:13.254824Z","shell.execute_reply.started":"2023-07-02T14:13:13.254588Z","shell.execute_reply":"2023-07-02T14:13:13.254608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we should only have saved the best checkpoint\ncheckpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\nassert len(checkpoints) == 1\n\ncheckpoints","metadata":{"_uuid":"96bea454-9e8e-4646-8225-ac33e72e8f3c","_cell_guid":"1d284037-9db9-45e2-8af3-3646075860bc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.256352Z","iopub.status.idle":"2023-07-02T14:13:13.256975Z","shell.execute_reply.started":"2023-07-02T14:13:13.256736Z","shell.execute_reply":"2023-07-02T14:13:13.256759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = hateful_memes_model.make_submission_frame(test_path)\nsubmission.head()","metadata":{"_uuid":"f29ea7cf-cbf8-4d77-baea-c5aa6594e064","_cell_guid":"e2fdafc5-c801-48d7-8584-309d0e2d376c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.258364Z","iopub.status.idle":"2023-07-02T14:13:13.258897Z","shell.execute_reply.started":"2023-07-02T14:13:13.258663Z","shell.execute_reply":"2023-07-02T14:13:13.258683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.groupby(\"label\").proba.mean()","metadata":{"_uuid":"7e51dd6a-5916-4082-9c6a-8cbcf21b8bae","_cell_guid":"23bd061d-3880-4e7c-a96a-24ecd9efb349","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.265152Z","iopub.status.idle":"2023-07-02T14:13:13.265684Z","shell.execute_reply.started":"2023-07-02T14:13:13.265449Z","shell.execute_reply":"2023-07-02T14:13:13.265484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.label.value_counts()","metadata":{"_uuid":"cd706ba9-84fb-49bc-a0e6-40f184d30ccf","_cell_guid":"7ab18f56-f45d-4c56-a799-48652fb35011","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.267724Z","iopub.status.idle":"2023-07-02T14:13:13.268219Z","shell.execute_reply.started":"2023-07-02T14:13:13.267996Z","shell.execute_reply":"2023-07-02T14:13:13.268017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv((\"model-outputs/submission.csv\"), index=True)","metadata":{"_uuid":"03e83b1a-2d24-41b2-8f28-08072ce10d50","_cell_guid":"7f61e566-2bc0-4f06-abe4-12007a25bbb8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.269836Z","iopub.status.idle":"2023-07-02T14:13:13.27053Z","shell.execute_reply.started":"2023-07-02T14:13:13.270265Z","shell.execute_reply":"2023-07-02T14:13:13.270286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have the submission frame with \"proba\" and \"label\" columns\nproba = torch.tensor(submission['proba'].values)\nlabel = torch.tensor(test['label'].values)\n\n# Calculate AUC-ROC score\nauc_roc = roc_auc_score(label, proba)\nprint(f\"AUC-ROC Score: {auc_roc}\")\n\n# Calculate accuracy\npredictions = proba.round().long()\naccuracy = accuracy_score(label, predictions)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"_uuid":"29118888-6bb3-4808-8c6a-6d54837db3cd","_cell_guid":"e0302e7f-b2fd-4347-b703-c778969e8ab2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.272166Z","iopub.status.idle":"2023-07-02T14:13:13.272816Z","shell.execute_reply.started":"2023-07-02T14:13:13.272583Z","shell.execute_reply":"2023-07-02T14:13:13.272604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_contour(study)","metadata":{"_uuid":"67194835-5e5c-4ce8-885c-907f9d4f102f","_cell_guid":"8e60d563-73ec-44c0-b299-b5467584b5eb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.274165Z","iopub.status.idle":"2023-07-02T14:13:13.274705Z","shell.execute_reply.started":"2023-07-02T14:13:13.274484Z","shell.execute_reply":"2023-07-02T14:13:13.274505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_intermediate_values(study)","metadata":{"_uuid":"c65b3ce6-fe7f-4535-a645-863ffb59a26e","_cell_guid":"1f373fcd-5f2f-4e37-a828-996534d62192","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.280161Z","iopub.status.idle":"2023-07-02T14:13:13.28076Z","shell.execute_reply.started":"2023-07-02T14:13:13.280524Z","shell.execute_reply":"2023-07-02T14:13:13.280544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"_uuid":"aa50ccd5-7fc9-4f0e-9478-fac4e77f05b5","_cell_guid":"952880a6-5e95-4777-9b09-c67e8d95103f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.282375Z","iopub.status.idle":"2023-07-02T14:13:13.282988Z","shell.execute_reply.started":"2023-07-02T14:13:13.282765Z","shell.execute_reply":"2023-07-02T14:13:13.282787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_parallel_coordinate(study)","metadata":{"_uuid":"67548bce-4127-4c1b-954c-ef00da2c8489","_cell_guid":"37169a1a-338b-4fb6-b8c7-75830c7da4cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.28455Z","iopub.status.idle":"2023-07-02T14:13:13.285072Z","shell.execute_reply.started":"2023-07-02T14:13:13.28484Z","shell.execute_reply":"2023-07-02T14:13:13.284861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_slice(study)","metadata":{"_uuid":"a5e596d0-a9af-43ef-9dcb-5f155a2a0695","_cell_guid":"f49e46ab-7a45-410a-9779-e7f2198a4132","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.287699Z","iopub.status.idle":"2023-07-02T14:13:13.288276Z","shell.execute_reply.started":"2023-07-02T14:13:13.288049Z","shell.execute_reply":"2023-07-02T14:13:13.28807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OCR (Optical Character Recognition)","metadata":{"_uuid":"8855c8b9-0817-41e9-a73e-ff339dcffae5","_cell_guid":"39c60389-dcba-4ef9-b05c-9742b7608716","trusted":true}},{"cell_type":"code","source":"!pip install pytesseract","metadata":{"_uuid":"016e3070-ca16-4981-b356-b167ca51161e","_cell_guid":"9f29a6bb-74a0-40ed-b37e-fe9e8ff8fd93","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.289731Z","iopub.status.idle":"2023-07-02T14:13:13.2903Z","shell.execute_reply.started":"2023-07-02T14:13:13.290076Z","shell.execute_reply":"2023-07-02T14:13:13.290097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytesseract\n\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\nimage_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n\n# Convert the image to grayscale\nimage = Image.open(image_path).convert(\"L\")\n\nimage = image.filter(ImageFilter.MedianFilter())\n\ntext = pytesseract.image_to_string(image, lang='eng')\ntext_without_newlines = text.replace('\\n', ' ')\nprint(text_without_newlines)","metadata":{"_uuid":"2a515988-fd38-4c52-875e-9388e1abb43b","_cell_guid":"23b031d8-0b77-46ff-9afd-f70e8853bb6e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.296172Z","iopub.status.idle":"2023-07-02T14:13:13.296708Z","shell.execute_reply.started":"2023-07-02T14:13:13.296481Z","shell.execute_reply":"2023-07-02T14:13:13.296503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install easyocr\n!pip install --upgrade opencv-python","metadata":{"_uuid":"304a9857-5054-455e-95e3-e13a66c9a499","_cell_guid":"b844f9e4-777c-4a8f-a685-132dd97e0537","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.298307Z","iopub.status.idle":"2023-07-02T14:13:13.298765Z","shell.execute_reply.started":"2023-07-02T14:13:13.298529Z","shell.execute_reply":"2023-07-02T14:13:13.298549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import easyocr\nimport cv2\n\nreader = easyocr.Reader(['en'],gpu=True)\n\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\nimage_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n\nresult = reader.readtext(image_path)\n\nextracted_text = []\nfor detection in result:\n    text = detection[1]\n    # Perform post-processing to handle \"o\" and \"0\" confusion\n    #text = text.replace('0', 'o')\n    extracted_text.append(text)\n\ntext = \" \".join(extracted_text)\n# Print the extracted words\nprint(text)","metadata":{"_uuid":"760dbcb9-6953-4e78-a874-75e66d321fbe","_cell_guid":"228801f8-e7c1-4aa2-947b-8d23ab82f2e8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.300451Z","iopub.status.idle":"2023-07-02T14:13:13.301108Z","shell.execute_reply.started":"2023-07-02T14:13:13.300872Z","shell.execute_reply":"2023-07-02T14:13:13.300893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install python-doctr","metadata":{"_uuid":"f8965ddc-ea15-43aa-a6ff-c3ce45ad5274","_cell_guid":"dc990d77-da2e-427b-b9ee-e727d2b8dcbf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.302752Z","iopub.status.idle":"2023-07-02T14:13:13.303286Z","shell.execute_reply.started":"2023-07-02T14:13:13.303059Z","shell.execute_reply":"2023-07-02T14:13:13.30308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall rapidfuzz -y\n!pip install rapidfuzz==2.15.1","metadata":{"_uuid":"4382bc44-0224-46c9-94ab-b7c38bf1805b","_cell_guid":"3da4f9ef-cf72-4e94-8995-1c7f4693b394","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.305088Z","iopub.status.idle":"2023-07-02T14:13:13.308808Z","shell.execute_reply.started":"2023-07-02T14:13:13.308439Z","shell.execute_reply":"2023-07-02T14:13:13.308491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall pillow -y\n!pip install pillow==7.1","metadata":{"_uuid":"e2dbe771-6fc2-4462-a2b2-c66424d54c98","_cell_guid":"cd662c08-73a2-4ed5-a7d4-c97e40e64ff0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.31031Z","iopub.status.idle":"2023-07-02T14:13:13.310865Z","shell.execute_reply.started":"2023-07-02T14:13:13.310623Z","shell.execute_reply":"2023-07-02T14:13:13.310643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tf2onnx","metadata":{"_uuid":"f5e9b92a-e9b1-4503-8e9f-d888ce65a567","_cell_guid":"f90bb377-9813-4ebe-9051-5b29d6b68dbc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.312461Z","iopub.status.idle":"2023-07-02T14:13:13.312988Z","shell.execute_reply.started":"2023-07-02T14:13:13.312764Z","shell.execute_reply":"2023-07-02T14:13:13.312784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from doctr.io import DocumentFile\nfrom doctr.models import ocr_predictor\n\nmodel = ocr_predictor(pretrained=True)\n\n# Load and process an image\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\n#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n\nsingle_img_doc = DocumentFile.from_images(image_path)\n\n# Perform OCR on the image\nresult = model(single_img_doc)\n\n#result.show(single_img_doc)","metadata":{"_uuid":"b8850f11-7074-476a-8261-9531fe8ef14f","_cell_guid":"a0aa4fb0-a7c0-4b0d-a02e-65f529c56fc0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.31451Z","iopub.status.idle":"2023-07-02T14:13:13.31612Z","shell.execute_reply.started":"2023-07-02T14:13:13.315851Z","shell.execute_reply":"2023-07-02T14:13:13.315874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the words from the Document object\nwords = []\nfor page in result.pages:\n    for block in page.blocks:\n        for line in block.lines:\n            for word in line.words:\n                words.append(word.value)\n                \ntext = \" \".join(words)\n# Print the extracted words\nprint(text)","metadata":{"_uuid":"e66b54a3-bada-4b0a-a8d8-2e2a455b1413","_cell_guid":"ce1e0e51-33f8-4751-88af-a85628ae5c3a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.32219Z","iopub.status.idle":"2023-07-02T14:13:13.322751Z","shell.execute_reply.started":"2023-07-02T14:13:13.32252Z","shell.execute_reply":"2023-07-02T14:13:13.322542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chceking classification accuracy on train","metadata":{"_uuid":"38c4ddad-80cc-4ed1-ac4d-e4b8c822550d","_cell_guid":"cbdb9ac6-fba9-4ead-9125-7539b97ca19a","trusted":true}},{"cell_type":"code","source":"def text_from_pytesseract(image_path):\n    # Convert the image to grayscale\n    image = Image.open(image_path).convert(\"L\")\n\n    image = image.filter(ImageFilter.MedianFilter())\n\n    text = pytesseract.image_to_string(image, lang='eng')\n    text_without_newlines = text.replace('\\n', ' ')\n    \n    return text_without_newlines","metadata":{"_uuid":"af25f492-ed09-45a1-ab19-074588826f69","_cell_guid":"1bf9d29f-06fa-4766-9b4a-cf55115d6019","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.324102Z","iopub.status.idle":"2023-07-02T14:13:13.324676Z","shell.execute_reply.started":"2023-07-02T14:13:13.324427Z","shell.execute_reply":"2023-07-02T14:13:13.324447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_from_easyocr_cv2(image_path):\n    reader = easyocr.Reader(['en'],gpu=True)\n    \n    result = reader.readtext(image_path)\n\n    extracted_text = []\n    for detection in result:\n        text = detection[1]\n        # Perform post-processing to handle \"o\" and \"0\" confusion\n        #text = text.replace('0', 'o')\n        extracted_text.append(text)\n\n    text = \" \".join(extracted_text)\n    \n    return text","metadata":{"_uuid":"93b32dcd-c065-4f29-9a62-2701c3f462f3","_cell_guid":"d262ae2d-eadd-4458-89e1-bcf97ba6f19b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.326449Z","iopub.status.idle":"2023-07-02T14:13:13.327093Z","shell.execute_reply.started":"2023-07-02T14:13:13.326869Z","shell.execute_reply":"2023-07-02T14:13:13.32689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_from_doctr(image_path):\n    single_img_doc = DocumentFile.from_images(image_path)\n\n    # Perform OCR on the image\n    result = model(single_img_doc)\n    \n    # Extract the words from the Document object\n    words = []\n    for page in result.pages:\n        for block in page.blocks:\n            for line in block.lines:\n                for word in line.words:\n                    words.append(word.value)\n                \n    text = \" \".join(words)\n    \n    return text","metadata":{"_uuid":"737e8867-f102-4ff4-99e3-1e3b2c47d9ab","_cell_guid":"fcb5ac21-b9f6-4ec8-9723-42947801b9c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.32839Z","iopub.status.idle":"2023-07-02T14:13:13.328965Z","shell.execute_reply.started":"2023-07-02T14:13:13.328736Z","shell.execute_reply":"2023-07-02T14:13:13.328757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef calc_cosine_similarity(text1, text2):\n    # Convert the texts into TF-IDF vectors\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform([text1, text2])\n\n    # Calculate the cosine similarity between the vectors\n    similarity = cosine_similarity(vectors)[0][1]\n    \n    return similarity","metadata":{"_uuid":"e2f77bd3-e5e6-4d17-9e35-22a2a9ef77e1","_cell_guid":"93acf54a-6bdd-4153-9548-836fc87945f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.330552Z","iopub.status.idle":"2023-07-02T14:13:13.335796Z","shell.execute_reply.started":"2023-07-02T14:13:13.335551Z","shell.execute_reply":"2023-07-02T14:13:13.335574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = []\npytesseract_cos_similarity = []\neasyocr_cv2_cos_similarity = []\ndoctr_cos_similarity = []\n\n# train from pd read json of train.\nfor index, row in train.head(100).iterrows():\n    text = row['text']\n    image = row['img']\n    \n    print(f\"{index}:{text}\")\n    texts.append(text)\n    \n    image_path = f\"/kaggle/input/hate-memes/hateful_memes/{image}\"\n    \n    # pytesseract\n    pytesseract_text = text_from_pytesseract(image_path)\n    \n    # print(pytesseract_text)\n    \n    pytesseract_cos_similarity.append(calc_cosine_similarity(text, pytesseract_text))\n        \n    # easyocr & cv2\n    easyocr_cv2_text = text_from_easyocr_cv2(image_path)\n    easyocr_cv2_cos_similarity.append(calc_cosine_similarity(text, easyocr_cv2_text))\n    \n    # print(easyocr_cv2_text)\n        \n    # doctr\n    doctr_text = text_from_doctr(image_path)\n    doctr_cos_similarity.append(calc_cosine_similarity(text, doctr_text))\n    \n    # print(doctr_text)\n        \n    print('######################################################')\n\nprint(f\"pytesseract average cosine similarity: {(sum(pytesseract_cos_similarity) / len(pytesseract_cos_similarity)):.4f}\")    \nprint(f\"asyocr & cv2 average cosine similarity: {(sum(easyocr_cv2_cos_similarity) / len(easyocr_cv2_cos_similarity)):.4f}\")\nprint(f\"doctr average cosine similarity: {(sum(doctr_cos_similarity) / len(doctr_cos_similarity)):.4f}\")","metadata":{"_uuid":"a132f61d-47f8-4f0f-acf1-f4b283c15770","_cell_guid":"1cb2b2aa-d383-437a-aaf0-5a16903dad6c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-07-02T14:13:13.337119Z","iopub.status.idle":"2023-07-02T14:13:13.337652Z","shell.execute_reply.started":"2023-07-02T14:13:13.337415Z","shell.execute_reply":"2023-07-02T14:13:13.337436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"da2f134f-3285-41c8-9263-a1c6baf78423","_cell_guid":"45ffe8a9-2fbe-43dc-a691-2a66e13f713b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"239a241a-2a99-479f-a0a6-7365da69630e","_cell_guid":"24342f6c-96b9-4673-89d1-02dd50b80672","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}