{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-05T19:38:14.047398Z","iopub.status.busy":"2023-06-05T19:38:14.047075Z"},"trusted":true},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install torchvision\n","!pip install pandas_path\n","!pip install pytorch-lightning==0.9.*\n","!pip install optuna\n","!pip install kornia"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import math\n","from typing import Tuple\n","import seaborn as sn\n","import pandas as pd\n","import pandas_path  # Path style access for pandas\n","import json\n","from pathlib import Path\n","import logging\n","import random\n","import tarfile\n","import tempfile\n","import warnings\n","# import fasttext\n","from transformers import BertTokenizerFast as BertTokenizer, BertModel, AutoConfig\n","\n","# pytorch\n","import torch\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","from torch.utils.data import dataset, WeightedRandomSampler, DataLoader\n","\n","import torchvision\n","from torchvision import transforms, utils\n","\n","from PIL import Image, ImageFilter, ImageEnhance\n","from tqdm import tqdm\n","\n","# optuna for hyperparameter optimization\n","import optuna\n","from optuna.trial import TrialState\n","from optuna.samplers import TPESampler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_dir = Path.cwd().parent / \"input\" / \"hate-memes\" / \"hateful_memes\"\n","\n","img_path = data_dir / \"img\"\n","train_path = data_dir / \"train.jsonl\"\n","dev_path = data_dir / \"dev_seen.jsonl\"\n","test_path = data_dir / \"test_seen.jsonl\""]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["train = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/train.jsonl\",lines=True)\n","val = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/dev_seen.jsonl\",lines=True)\n","test = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/test_seen.jsonl\",lines=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train.label.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train[\"label\"].value_counts().plot(kind=\"bar\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Blancing train set - to prevent biased model and poor performance.\n","We'll use oversampling for that."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_weights = [1.0,5481/3019]\n","sample_weights = [0] * len(train)\n","\n","for idx in range(len(train)) :\n","    class_weight = class_weights[train.label[idx]]\n","    sample_weights[idx] = class_weight\n","    \n","# replacement=True - we want to see the same sample\n","# more than once during training.\n","sampler = WeightedRandomSampler(sample_weights, \n","                                num_samples=len(sample_weights),\n","                                replacement=True)\n","# for later - after transform\n","# train_loader = DataLoader(data, batch_size=batch_size, sampler=sampler)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,6))\n","img = plt.imread(f\"/kaggle/input/hate-memes/hateful_memes/img/42953.png\")\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = [\n","    Image.open(f\"/kaggle/input/hate-memes/hateful_memes/{train.img[i]}\").convert(\"RGB\")\n","    for i in range(5)\n","]\n","\n","for image in images:\n","    print(image.size)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define a callable image_transform with Compose\n","image_transform = transforms.Compose(\n","    [\n","        # we'll need to resize the images to form tensor \n","        # minibatches appropriate for training a model.\n","        transforms.Resize(size=(224, 224)),\n","        transforms.ToTensor()\n","    ]\n",")\n","\n","# convert the images and prepare for visualization.\n","tensor_img = torch.stack(\n","    [image_transform(image) for image in images]\n",")\n","grid = utils.make_grid(tensor_img)\n","\n","# plot\n","plt.rcParams[\"figure.figsize\"] = (20, 5)\n","plt.axis('off')\n","_ = plt.imshow(grid.permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class HatefulMemesDataset(torch.utils.data.Dataset):\n","    \"\"\"Uses jsonl data to preprocess and serve \n","    dictionary of multimodal tensors for model input.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        data_path,\n","        img_dir,\n","        image_transform,\n","        text_transform,\n","        balance=False,\n","        dev_limit=None,\n","        random_state=0,\n","    ):\n","\n","        self.samples_frame = pd.read_json(\n","            data_path, lines=True\n","        )\n","        self.dev_limit = dev_limit\n","        if balance:\n","            neg = self.samples_frame[\n","                self.samples_frame.label.eq(0)\n","            ]\n","            pos = self.samples_frame[\n","                self.samples_frame.label.eq(1)\n","            ]\n","            self.samples_frame = pd.concat(\n","                [\n","                    neg.sample(\n","                        pos.shape[0], \n","                        random_state=random_state\n","                    ), \n","                    pos\n","                ]\n","            )\n","        if self.dev_limit:\n","            if self.samples_frame.shape[0] > self.dev_limit:\n","                self.samples_frame = self.samples_frame.sample(\n","                    dev_limit, random_state=random_state\n","                )\n","        self.samples_frame = self.samples_frame.reset_index(\n","            drop=True\n","        )\n","        self.samples_frame.img = self.samples_frame.apply(\n","            lambda row: (img_dir / row.img), axis=1\n","        )\n","\n","        # https://github.com/drivendataorg/pandas-path\n","        if not self.samples_frame.img.apply(lambda x: (img_dir / x).exists()).all():\n","            raise FileNotFoundError\n","        if not (self.samples_frame.img.apply(lambda x: (img_dir / x).is_file())).all():\n","            raise TypeError\n","            \n","        self.image_transform = image_transform\n","        self.text_transform = text_transform\n","\n","    def __len__(self):\n","        \"\"\"This method is called when you do len(instance) \n","        for an instance of this class.\n","        \"\"\"\n","        return len(self.samples_frame)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"This method is called when you do instance[key] \n","        for an instance of this class.\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_id = self.samples_frame.loc[idx, \"id\"]\n","\n","        image = Image.open(\n","            self.samples_frame.loc[idx, \"img\"]\n","        ).convert(\"RGB\")\n","        image = self.image_transform(image)\n","\n","        text = torch.Tensor(\n","            self.text_transform.encode_plus(\n","                self.samples_frame.loc[idx, \"text\"],\n","                add_special_tokens=True,\n","                max_length=512,\n","                pad_to_max_length=True,\n","            )[\"input_ids\"]\n","        ).long().squeeze()\n","        \n","\n","        if \"label\" in self.samples_frame.columns:\n","            label = torch.Tensor(\n","                [self.samples_frame.loc[idx, \"label\"]]\n","            ).long().squeeze()\n","            sample = {\n","                \"id\": img_id, \n","                \"image\": image, \n","                \"text\": text, \n","                \"label\": label\n","            }\n","        else:\n","            sample = {\n","                \"id\": img_id, \n","                \"image\": image, \n","                \"text\": text\n","            }\n","\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LanguageAndVisionConcat(torch.nn.Module):\n","    def __init__(\n","        self,\n","        num_classes,\n","        loss_fn,\n","        language_module,\n","        post_language_module,\n","        vision_module,\n","        language_feature_dim,\n","        vision_feature_dim,\n","        fusion_mid_size,\n","        fusion_output_size,\n","        dropout_p,\n","    ):\n","        for i in range(10) print(language_feature_dim + vision_feature_dim)\n","        super(LanguageAndVisionConcat, self).__init__()\n","        self.language_module = language_module\n","        self.vision_module = vision_module\n","        self.post_language_module = post_language_module\n","        self.fusion = torch.nn.sequential(\n","            torch.nn.Linear(\n","                in_features=language_feature_dim + vision_feature_dim,\n","                out_features=fusion_mid_size\n","            ),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(\n","                in_features=fusion_mid_size,\n","                out_features=fusion_output_size\n","            )\n","        )\n","        self.fc = torch.nn.Linear(\n","            in_features=fusion_output_size, \n","            out_features=num_classes\n","        )\n","        self.loss_fn = loss_fn\n","        self.dropout = torch.nn.Dropout(dropout_p)\n","        \n","    def forward(self, text, image, label=None):\n","        text_features = torch.nn.functional.relu(\n","            self.language_module(text)[0][:, 0, :]\n","        )\n","        text_features = self.post_language_module(text_features)\n","        image_features = torch.nn.functional.relu(\n","            self.vision_module(image)   \n","        )\n","        combined = torch.cat(\n","            [text_features.squeeze(), image_features], dim=1\n","        )\n","        fused = self.dropout(\n","            torch.nn.functional.relu(\n","            self.fusion(combined)\n","            )\n","        )\n","        logits = self.fc(fused)\n","        pred = torch.nn.functional.softmax(logits)\n","        loss = (\n","            self.loss_fn(pred, label) \n","            if label is not None else label\n","        )\n","        return (pred, loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pytorch_lightning as pl\n","\n","# for the purposes of this post, we'll filter\n","# much of the lovely logging info from our LightningModule\n","warnings.filterwarnings(\"ignore\")\n","logging.getLogger().setLevel(logging.WARNING)\n","\n","\n","class HatefulMemesModel(pl.LightningModule):\n","    def __init__(self, hparams):\n","        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n","            # ok, there's one for-loop but it doesn't count\n","            if data_key not in hparams.keys():\n","                raise KeyError(\n","                    f\"{data_key} is a required hparam in this model\"\n","                )\n","        \n","        super(HatefulMemesModel, self).__init__()\n","        self.hparams = hparams\n","        \n","        # assign some hparams that get used in multiple places\n","        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n","        self.language_feature_dim = self.hparams.get(\n","            \"language_feature_dim\", 300\n","        )\n","        self.vision_feature_dim = self.hparams.get(\n","            # balance language and vision features by default\n","            \"vision_feature_dim\", self.language_feature_dim\n","        )\n","        self.output_path = Path(\n","            self.hparams.get(\"output_path\", \"model-outputs\")\n","        )\n","        self.output_path.mkdir(exist_ok=True)\n","        \n","        # instantiate transforms, datasets\n","        self.text_transform = self._build_text_transform()\n","        self.image_transform = self._build_image_transform()\n","        self.train_dataset = self._build_dataset(\"train_path\")\n","        self.dev_dataset = self._build_dataset(\"dev_path\")\n","        \n","        # set up model and training\n","        self.model = self._build_model()\n","        self.trainer_params = self._get_trainer_params()\n","    \n","    ## Required LightningModule Methods (when validating) ##\n","    \n","    def forward(self, text, image, label=None):\n","        return self.model(text, image, label)\n","\n","    def training_step(self, batch, batch_nb):\n","        # Use mixed precision training to leverage hardware \n","        # acceleration through mixed precision computations.\n","        preds, loss = self.forward(\n","            text=batch[\"text\"], \n","            image=batch[\"image\"], \n","            label=batch[\"label\"]\n","        )\n","        \n","        return {\"loss\": loss}\n","\n","    def validation_step(self, batch, batch_nb):\n","        preds, loss = self.eval().forward(\n","            text=batch[\"text\"], \n","            image=batch[\"image\"], \n","            label=batch[\"label\"]\n","        )\n","        \n","        return {\"batch_val_loss\": loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack(\n","            tuple(\n","                output[\"batch_val_loss\"] \n","                for output in outputs\n","            )\n","        ).mean()\n","        \n","        return {\n","            \"val_loss\": avg_loss,\n","            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n","        }\n","\n","    def configure_optimizers(self):\n","        optimizers = [\n","            torch.optim.AdamW(\n","                self.model.parameters(), \n","                lr=self.hparams.get(\"lr\", 0.001)\n","            )\n","        ]\n","        schedulers = [\n","            torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                optimizers[0]\n","            )\n","        ]\n","        return optimizers, schedulers\n","    \n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.train_dataset, \n","            shuffle=True, \n","            batch_size=self.hparams.get(\"batch_size\", 4), \n","            num_workers=self.hparams.get(\"num_workers\", 16)\n","        )\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.dev_dataset, \n","            shuffle=False, \n","            batch_size=self.hparams.get(\"batch_size\", 4), \n","            num_workers=self.hparams.get(\"num_workers\", 16)\n","        )\n","    \n","    ## Convenience Methods ##\n","    \n","    def fit(self):\n","        self._set_seed(self.hparams.get(\"random_state\", 42))\n","        self.trainer = pl.Trainer(**self.trainer_params)\n","        self.trainer.fit(self)\n","        \n","        \n","    def _set_seed(self, seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","    def _build_text_transform(self):\n","        with tempfile.NamedTemporaryFile() as ft_training_data:\n","            ft_path = Path(ft_training_data.name)\n","            with ft_path.open(\"w\") as ft:\n","                training_data = [\n","                    json.loads(line)[\"text\"] + \"/n\" \n","                    for line in open(\n","                        self.hparams.get(\"train_path\")\n","                    ).read().splitlines()\n","                ]\n","                for line in training_data:\n","                    ft.write(line + \"\\n\")\n","        ### using bert tokenizer ###\n","                text_transform = BertTokenizer.from_pretrained(\n","                    \"bert-base-uncased\",\n","                    do_lower_case=True\n","                )\n","        return text_transform\n","\n","        ### using FastText embedding ###\n","        #         text_transform = fasttext.train_unsupervised(\n","        #             str(ft_path),\n","        #             model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n","        #             dim=self.embedding_dim\n","        #         )\n","        # return text_transform\n","        \n","    \n","    def _build_image_transform(self):\n","        image_dim = self.hparams.get(\"image_dim\", 224)\n","        image_transform = torchvision.transforms.Compose(\n","            [\n","                torchvision.transforms.Resize(\n","                    size=(image_dim, image_dim)\n","                ),        \n","                torchvision.transforms.ToTensor(),\n","                # all torchvision models expect the same\n","                # normalization mean and std\n","                # https://pytorch.org/docs/stable/torchvision/models.html\n","                torchvision.transforms.Normalize(\n","                    mean=(0.485, 0.456, 0.406), \n","                    std=(0.229, 0.224, 0.225)\n","                ),\n","            ]\n","        )\n","        return image_transform\n","\n","    def _build_dataset(self, dataset_key):\n","        return HatefulMemesDataset(\n","            data_path=self.hparams.get(dataset_key, dataset_key),\n","            img_dir=self.hparams.get(\"img_dir\"),\n","            image_transform=self.image_transform,\n","            text_transform=self.text_transform,\n","            # limit training samples only\n","            dev_limit=(\n","                self.hparams.get(\"dev_limit\", None) \n","                if \"train\" in str(dataset_key) else None\n","            ),\n","            balance=True if \"train\" in str(dataset_key) else False,\n","        )\n","    \n","    def _build_model(self):\n","        # configure bert model\n","        configuration = AutoConfig.from_pretrained(\n","            \"bert-base-uncased\",\n","            # dropout\n","            hidden_dropout_prob=self.hparams.get(\"bert_hidden_dropout_p\", 0.1),\n","            attention_probs_dropout_prob=self.hparams.get(\"bert_attn_dropout_p\", 0.1),\n","            output_hidden_states=True, # return hidden states\n","        ) \n","        # load pretrained bert\n","        language_module = BertModel.from_pretrained(\n","            \"bert-base-uncased\",\n","            config=configuration\n","        )\n","        \n","        # we want to fine-tune the language module\n","        for param in language_module.parameters():\n","            param.requires_grad = True\n","\n","        # the out of language_module will be the input of FC layer\n","        language_module.pooler = torch.nn.Linear(\n","            in_features=768, # bert hidden size\n","            out_features=self.language_feature_dim\n","        )\n","        \n","\n","\n","\n","        # load pretrained resnet152\n","        vision_module = torchvision.models.resnet152(\n","            pretrained=True\n","        )\n","        vision_module.fc = torch.nn.Linear(\n","                in_features=2048,\n","                out_features=self.vision_feature_dim\n","        )\n","\n","        return LanguageAndVisionConcat(\n","            num_classes=self.hparams.get(\"num_classes\", 2),\n","            loss_fn=torch.nn.CrossEntropyLoss(),\n","            language_module=language_module, # the output of language_module is a tuple where the first value is the last_hidden_state\n","            post_language_module=language_module.pooler,\n","            vision_module=vision_module,\n","            language_feature_dim=self.language_feature_dim,\n","            vision_feature_dim=self.vision_feature_dim,\n","            fusion_mid_size=self.hparams.get(\"fusion_mid_size\", 1024),\n","            fusion_output_size=self.hparams.get(\n","                \"fusion_output_size\", 512\n","            ),\n","            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n","        )\n","    \n","    def _get_trainer_params(self):\n","        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","            filepath=self.output_path,\n","            monitor=self.hparams.get(\n","                \"checkpoint_monitor\", \"avg_val_loss\"\n","            ),\n","            mode=self.hparams.get(\n","                \"checkpoint_monitor_mode\", \"min\"\n","            ),\n","            verbose=self.hparams.get(\"verbose\", True)\n","        )\n","\n","        early_stop_callback = pl.callbacks.EarlyStopping(\n","            monitor=self.hparams.get(\n","                \"early_stop_monitor\", \"avg_val_loss\"\n","            ),\n","            min_delta=self.hparams.get(\n","                \"early_stop_min_delta\", 0.001\n","            ),\n","            patience=self.hparams.get(\n","                \"early_stop_patience\", 3\n","            ),\n","            verbose=self.hparams.get(\"verbose\", True),\n","        )\n","\n","        trainer_params = {\n","            \"checkpoint_callback\": checkpoint_callback,\n","            \"early_stop_callback\": early_stop_callback,\n","            \"default_root_dir\": self.output_path,\n","            \"accumulate_grad_batches\": self.hparams.get(\n","                \"accumulate_grad_batches\", 1\n","            ),\n","            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n","            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n","            \"gradient_clip_val\": self.hparams.get(\n","                \"gradient_clip_value\", 1\n","            ),\n","        }\n","        return trainer_params\n","            \n","    @torch.no_grad()\n","    def make_submission_frame(self, test_path):\n","        test_dataset = self._build_dataset(test_path)\n","        submission_frame = pd.DataFrame(\n","            index=test_dataset.samples_frame.id,\n","            columns=[\"proba\", \"label\"]\n","        )\n","        test_dataloader = torch.utils.data.DataLoader(\n","            test_dataset, \n","            shuffle=False, \n","            batch_size=self.hparams.get(\"batch_size\", 4), \n","            num_workers=self.hparams.get(\"num_workers\", 16))\n","        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n","            preds, _ = self.model.eval().to(\"cpu\")(\n","                batch[\"text\"], batch[\"image\"]\n","            )\n","            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n","            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n","        submission_frame.proba = submission_frame.proba.astype(float)\n","        submission_frame.label = submission_frame.label.astype(int)\n","        return submission_frame\n","    \n","    \n","\"\"\"\n","the architecture of the model:\n","    # 1. text_transform: FastText num of parameters: 0 because we use the pretrained model without fine-tuning\n","    2. image_transform: ResNet152 num of parameters: 58,279,234\n","    3. language_module: Linear(in_features=300, out_features=512, bias=True) num of parameters: 153,600\n","    4. vision_module: Linear(in_features=2048, out_features=512, bias=True) num of parameters: 1,049,088\n","    5. fusion: Linear(in_features=1024, out_features=512, bias=True) num of parameters: 524,800\n","    6. output: Linear(in_features=512, out_features=2, bias=True) num of parameters: 1,026\n","    total num of parameters: 59,007,748\n","\"\"\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hparams = {\n","    \n","    # Required hparams\n","    \"train_path\": train_path,\n","    \"dev_path\": dev_path,\n","    \"img_dir\": data_dir,\n","    \n","    # Optional hparams\n","    \"embedding_dim\": 150,\n","    \"language_feature_dim\": 300,\n","    \"vision_feature_dim\": 300,\n","    \"fusion_mid_size\": 256, \n","    \"fusion_output_size\": 256,\n","    \"output_path\": \"model-outputs\",\n","    \"dev_limit\": None,\n","    \"lr\": 0.00005,\n","    \"max_epochs\": 4,\n","    \"n_gpu\": 1, # torch.cuda.device_count(),\n","    \"batch_size\": 4,\n","    # allows us to \"simulate\" having larger batches \n","    \"accumulate_grad_batches\": 16, \n","    \"early_stop_patience\": 3,\n","    \n","}\n","\n","#hateful_memes_model = HatefulMemesModel(hparams=hparams)\n","#hateful_memes_model.(fit)()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# use optuna to tune hyperparameters\n","# define objective function\n","def objective(trial):\n","    # sample hpsearch params\n","    hparams[\"lr\"] = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n","    hparams[\"dropout_p\"] = trial.suggest_float(\"dropout_p\", 0, 0.5, step=0.01)\n","    hparams[\"bert_hidden_dropout_p\"] = trial.suggest_float(\"bert_hidden_dropout_p\", 0, 0.5, step=0.01)\n","    hparams[\"bert_attn_dropout_p\"] = trial.suggest_float(\"bert_attn_dropout_p\", 0, 0.5, step=0.01)\n","    hparams[\"accumulate_grad_batches\"] = trial.suggest_categorical(\n","        \"accumulate_grad_batches\", [1, 2, 4, 8, 16, 32]\n","    )\n","    hparams[\"batch_size\"] = trial.suggest_categorical(\n","        \"batch_size\", [4, 8, 16, 32, 64]\n","    )\n","    hparams[\"language_feature_dim\"] = trial.suggest_int(\n","        \"language_feature_dim\", 100, 500, step=10\n","    )\n","    hparams[\"vision_feature_dim\"] = trial.suggest_int(\n","        \"vision_feature_dim\", 100, 500, step=10\n","    )\n","    hparams[\"fusion_output_size\"] = trial.suggest_int(\n","        \"fusion_output_size\", 100, 500, step=10\n","    )\n","\n","    \n","    # train model\n","    hateful_memes_model = HatefulMemesModel(hparams=hparams)\n","    hateful_memes_model.fit()\n","    \n","    # report results back to optuna\n","    return hateful_memes_model.trainer.callback_metrics[\"val_loss\"].item()\n","\n","\n","# define optuna params\n","study = optuna.create_study(direction=\"minimize\")\n","\n","# optimize model\n","study.optimize(objective, n_trials=100, timeout=3600, show_progress_bar=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# we should only have saved the best checkpoint\n","checkpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\n","assert len(checkpoints) == 1\n","\n","checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\n","    checkpoints[0].as_posix()\n",")\n","submission = hateful_memes_model.make_submission_frame(\n","    test_path\n",")\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.groupby(\"label\").proba.mean()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.label.value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv((\"model-outputs/submission.csv\"), index=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, accuracy_score\n","\n","# Assuming you have the submission frame with \"proba\" and \"label\" columns\n","proba = torch.tensor(submission['proba'].values)\n","label = torch.tensor(test['label'].values)\n","\n","# Calculate AUC-ROC score\n","auc_roc = roc_auc_score(label, proba)\n","print(f\"AUC-ROC Score: {auc_roc}\")\n","\n","# Calculate accuracy\n","predictions = proba.round().long()\n","accuracy = accuracy_score(label, predictions)\n","print(f\"Accuracy: {accuracy}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["OCR (Optical Character Recognition) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install pytesseract"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pytesseract\n","\n","#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n","#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\n","image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n","\n","# Convert the image to grayscale\n","image = Image.open(image_path).convert(\"L\")\n","\n","image = image.filter(ImageFilter.MedianFilter())\n","\n","text = pytesseract.image_to_string(image, lang='eng')\n","\n","print(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade easyocr\n","!pip install --upgrade opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import easyocr\n","import cv2\n","\n","reader = easyocr.Reader(['en'])\n","\n","#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/42953.png\"\n","#image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01236.png\"\n","image_path = \"/kaggle/input/hate-memes/hateful_memes/img/01243.png\"\n","\n","result = reader.readtext(image_path)\n","\n","extracted_text = []\n","for detection in result:\n","    text = detection[1]\n","    # Perform post-processing to handle \"o\" and \"0\" confusion\n","    #text = text.replace('0', 'o')\n","    extracted_text.append(text)\n","    \n","for text in extracted_text:\n","    print(text)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
