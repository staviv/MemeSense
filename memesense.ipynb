{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install torchvision\n!pip install pandas_path\n!pip install pytorch-lightning==0.9.*\n!pip install optuna\n!pip install joblib","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-06-12T22:39:32.579894Z","iopub.status.idle":"2023-06-12T22:39:32.580865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport math\nfrom typing import Tuple\nimport seaborn as sn\nimport pandas as pd\nimport pandas_path  # Path style access for pandas\nimport json\nfrom pathlib import Path\nimport logging\nimport random\nimport tarfile\nimport tempfile\nimport warnings\n# import transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n# pytorch\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset, WeightedRandomSampler, DataLoader\n\nimport torchvision\nfrom torchvision import transforms, utils\n\nfrom PIL import Image, ImageFilter, ImageEnhance\nfrom tqdm import tqdm\n\nimport pytorch_lightning as pl\n\n# optuna for hyperparameter optimization\nimport optuna\nfrom optuna.trial import TrialState\nfrom optuna.samplers import TPESampler\nimport joblib\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.953600Z","iopub.status.idle":"2023-06-12T22:39:21.954079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getTokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"distilbert-base-uncased\",\n        do_lower_case=True\n    )\n    return tokenizer\n\ndef gettextmodel():\n    text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n    return text_model\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.955839Z","iopub.status.idle":"2023-06-12T22:39:21.956733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ndata_dir = Path.cwd().parent / \"input\" / \"hate-memes\" / \"hateful_memes\"\n\nimg_path = data_dir / \"img\"\ntrain_path = data_dir / \"train.jsonl\"\ndev_path = data_dir / \"dev_seen.jsonl\"\ntest_path = data_dir / \"test_seen.jsonl\"","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.958148Z","iopub.status.idle":"2023-06-12T22:39:21.958900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/train.jsonl\",lines=True)\nval = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/dev_seen.jsonl\",lines=True)\ntest = pd.read_json(\"/kaggle/input/hate-memes/hateful_memes/test_seen.jsonl\",lines=True)\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2023-06-12T22:39:21.960428Z","iopub.status.idle":"2023-06-12T22:39:21.961141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.962633Z","iopub.status.idle":"2023-06-12T22:39:21.963382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.964840Z","iopub.status.idle":"2023-06-12T22:39:21.965632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"label\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.967170Z","iopub.status.idle":"2023-06-12T22:39:21.967892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Blancing train set - to prevent biased model and poor performance.\nWe'll use oversampling for that.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nimg = plt.imread(f\"/kaggle/input/hate-memes/hateful_memes/img/42953.png\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.969481Z","iopub.status.idle":"2023-06-12T22:39:21.970257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [\n    Image.open(f\"/kaggle/input/hate-memes/hateful_memes/{train.img[i]}\").convert(\"RGB\")\n    for i in range(5)\n]\n\nfor image in images:\n    print(image.size)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.971736Z","iopub.status.idle":"2023-06-12T22:39:21.972528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a callable image_transform with data augmentation\nimage_transform = transforms.Compose(\n    [\n        # we'll need to resize the images to form tensor \n        # minibatches appropriate for training a model.\n        transforms.Resize(size=(224, 224)),\n        #transforms.RandomHorizontalFlip(),\n        #transforms.RandomRotation(degrees=10),\n        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n        transforms.ToTensor()\n    ]\n)\n\n# convert the images and prepare for visualization.\ntensor_img = torch.stack(\n    [image_transform(image) for image in images]\n)\ngrid = utils.make_grid(tensor_img)\n\n# plot\nplt.rcParams[\"figure.figsize\"] = (20, 5)\nplt.axis('off')\n_ = plt.imshow(grid.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.974089Z","iopub.status.idle":"2023-06-12T22:39:21.974860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HatefulMemesDataset(torch.utils.data.Dataset):\n    \"\"\"Uses jsonl data to preprocess and serve \n    dictionary of multimodal tensors for model input.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        img_dir,\n        image_transform,\n        text_transform,\n        balance=False,\n        dev_limit=None,\n        random_state=0,\n    ):\n\n        self.samples_frame = pd.read_json(\n            data_path, lines=True\n        )\n        self.dev_limit = dev_limit\n        if balance:\n            neg = self.samples_frame[\n                self.samples_frame.label.eq(0)\n            ]\n            pos = self.samples_frame[\n                self.samples_frame.label.eq(1)\n            ]\n            self.samples_frame = pd.concat(\n                [\n                    neg.sample(\n                        pos.shape[0], \n                        random_state=random_state\n                    ), \n                    pos\n                ]\n            )\n        if self.dev_limit:\n            if self.samples_frame.shape[0] > self.dev_limit:\n                self.samples_frame = self.samples_frame.sample(\n                    dev_limit, random_state=random_state\n                )\n        self.samples_frame = self.samples_frame.reset_index(\n            drop=True\n        )\n        self.samples_frame.img = self.samples_frame.apply(\n            lambda row: (img_dir / row.img), axis=1\n        )\n\n        # https://github.com/drivendataorg/pandas-path\n        if not self.samples_frame.img.apply(lambda x: (img_dir / x).exists()).all():\n            raise FileNotFoundError\n        if not (self.samples_frame.img.apply(lambda x: (img_dir / x).is_file())).all():\n            raise TypeError\n            \n        self.image_transform = image_transform\n        self.text_transform = text_transform\n\n    def __len__(self):\n        \"\"\"This method is called when you do len(instance) \n        for an instance of this class.\n        \"\"\"\n        return len(self.samples_frame)\n\n    def __getitem__(self, idx):\n        \"\"\"This method is called when you do instance[key] \n        for an instance of this class.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_id = self.samples_frame.loc[idx, \"id\"]\n\n        image = Image.open(\n            self.samples_frame.loc[idx, \"img\"]\n        ).convert(\"RGB\")\n        image = self.image_transform(image)\n\n        text = torch.Tensor(\n            self.text_transform.encode_plus(\n                self.samples_frame.loc[idx, \"text\"],\n                add_special_tokens=True,\n                max_length=512,\n                pad_to_max_length=True,\n            )[\"input_ids\"]\n        ).long().squeeze()\n        \n\n        if \"label\" in self.samples_frame.columns:\n            label = torch.Tensor(\n                [self.samples_frame.loc[idx, \"label\"]]\n            ).long().squeeze()\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text, \n                \"label\": label\n            }\n        else:\n            sample = {\n                \"id\": img_id, \n                \"image\": image, \n                \"text\": text\n            }\n\n        return sample","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.977538Z","iopub.status.idle":"2023-06-12T22:39:21.978308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LanguageAndVisionConcat(torch.nn.Module):\n    def __init__(\n        self,\n        num_classes,\n        loss_fn,\n        language_module,\n        post_language_module,\n        vision_module,\n        language_feature_dim,\n        vision_feature_dim,\n        fusion_mid_size,\n        fusion_output_size,\n        dropout_vision_feature_p,\n        dropout_fusion_in_p,\n        dropout_fusion_mid_p,\n        dropout_fusion_out_p,\n        dropout_text_feature_p\n    ):\n        super(LanguageAndVisionConcat, self).__init__()\n        self.language_module = language_module\n        self.post_language_module = post_language_module\n        self.text_feature_dropout = torch.nn.Dropout(dropout_text_feature_p)\n        self.vision_module = vision_module\n        self.vision_feature_dropout = torch.nn.Dropout(dropout_vision_feature_p)\n        self.fusion = torch.nn.Sequential(\n            torch.nn.Dropout(dropout_fusion_in_p),\n            torch.nn.Linear(\n                in_features=language_feature_dim+vision_feature_dim,\n                out_features=fusion_mid_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Linear(in_features=fusion_mid_size, out_features=fusion_mid_size),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_fusion_mid_p),\n            torch.nn.Linear(\n                in_features=fusion_mid_size,\n                out_features=fusion_output_size\n            ),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_fusion_out_p)\n        )\n        self.fc = torch.nn.Linear(\n            in_features=fusion_output_size, \n            out_features=num_classes\n        )\n        \n        self.loss_fn = loss_fn\n        \n    def forward(self, text, image, label=None):\n        text_features = torch.nn.functional.relu(\n            self.language_module(text)[0][:, 0, :]\n        )\n        text_features = torch.nn.functional.relu(\n            self.text_feature_dropout(\n                self.post_language_module(text_features)\n            )\n        )\n        image_features = torch.nn.functional.relu(\n            self.vision_feature_dropout(\n                self.vision_module(image)\n            )\n        )\n        combined = torch.cat(\n            [text_features.squeeze(), image_features], dim=1\n        )\n        fused = self.fusion(combined)\n        logits = self.fc(fused)\n        pred = torch.nn.functional.softmax(logits)\n        loss = (\n            self.loss_fn(pred, label) \n            if label is not None else label\n        )\n        return (pred, loss)","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.979939Z","iopub.status.idle":"2023-06-12T22:39:21.980753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# for the purposes of this post, we'll filter\n# much of the lovely logging info from our LightningModule\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger().setLevel(logging.WARNING)\n\n\nclass HatefulMemesModel(pl.LightningModule):\n    def __init__(self, hparams):\n        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n            # ok, there's one for-loop but it doesn't count\n            if data_key not in hparams.keys():\n                raise KeyError(\n                    f\"{data_key} is a required hparam in this model\"\n                )\n        \n        super(HatefulMemesModel, self).__init__()\n        self.hparams = hparams\n        \n        # assign some hparams that get used in multiple places\n        #self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n        self.language_feature_dim = self.hparams.get(\n            \"language_feature_dim\", 300\n        )\n        self.vision_feature_dim = self.hparams.get(\n            # balance language and vision features by default\n            \"vision_feature_dim\", self.language_feature_dim\n        )\n        self.output_path = Path(\n            self.hparams.get(\"output_path\", \"model-outputs\")\n        )\n        self.output_path.mkdir(exist_ok=True)\n        \n        # instantiate transforms, datasets\n        self.text_transform = self._build_text_transform()\n        self.image_transform = self._build_image_transform()\n        self.train_dataset = self._build_dataset(\"train_path\")\n        self.dev_dataset = self._build_dataset(\"dev_path\")\n        \n        # set up model and training\n        self.model = self._build_model()\n        self.trainer_params = self._get_trainer_params()\n    \n    ## Required LightningModule Methods (when validating) ##\n    \n    def forward(self, text, image, label=None):\n        return self.model(text, image, label)\n\n    def training_step(self, batch, batch_nb):\n        # Use mixed precision training to leverage hardware \n        # acceleration through mixed precision computations.\n        preds, loss = self.forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        \n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_nb):\n        preds, loss = self.eval().forward(\n            text=batch[\"text\"], \n            image=batch[\"image\"], \n            label=batch[\"label\"]\n        )\n        preds = (preds[:,0] > 0.5)\n        labels = batch[\"label\"]\n        loss = (preds!=labels).float().mean()\n        \n        return {\"batch_val_loss\": loss}\n\n    def validation_epoch_end(self, outputs):\n        \n        avg_loss = torch.stack(\n            tuple(\n                output[\"batch_val_loss\"] \n                for output in outputs\n            )\n        ).mean()\n        \n        return {\n            \"val_loss\": avg_loss,\n            \"progress_bar\":{\"avg_val_loss\": avg_loss}\n        }\n\n    def configure_optimizers(self):\n        optimizers = [\n            torch.optim.AdamW(\n                self.model.parameters(), \n                lr=self.hparams.get(\"lr\", 0.001)\n            )\n        ]\n        schedulers = [\n            torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizers[0]\n            )\n        ]\n        return optimizers, schedulers\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset, \n            shuffle=True, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.dev_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 4), \n            num_workers=self.hparams.get(\"num_workers\", 16)\n        )\n    \n    ## Convenience Methods ##\n    \n    def fit(self):\n        self._set_seed(self.hparams.get(\"random_state\", 42))\n        self.trainer = pl.Trainer(**self.trainer_params)\n        self.trainer.fit(self)\n        \n        \n    def _set_seed(self, seed):\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n\n    def _build_text_transform(self):\n        with tempfile.NamedTemporaryFile() as ft_training_data:\n            ft_path = Path(ft_training_data.name)\n            with ft_path.open(\"w\") as ft:\n                training_data = [\n                    json.loads(line)[\"text\"] + \"/n\" \n                    for line in open(\n                        self.hparams.get(\"train_path\")\n                    ).read().splitlines()\n                ]\n                for line in training_data:\n                    ft.write(line + \"\\n\")\n        ### using the tokenizer ###\n                text_transform = getTokenizer()\n        return text_transform\n\n        ### using FastText embedding ###\n        #         text_transform = fasttext.train_unsupervised(\n        #             str(ft_path),\n        #             model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n        #             dim=self.embedding_dim\n        #         )\n        # return text_transform\n        \n    \n    def _build_image_transform(self):\n        image_dim = self.hparams.get(\"image_dim\", 224)\n        # Define the transformations for data augmentation\n        image_transform = transforms.Compose(\n        [\n        # we'll need to resize the images to form tensor \n        # minibatches appropriate for training a model.\n        transforms.Resize(size=(image_dim, image_dim)),\n        #transforms.RandomHorizontalFlip(),\n        #transforms.RandomRotation(degrees=10),\n        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n        transforms.ToTensor()\n    ]\n)\n        return image_transform\n\n    def _build_dataset(self, dataset_key):\n        return HatefulMemesDataset(\n            data_path=self.hparams.get(dataset_key, dataset_key),\n            img_dir=self.hparams.get(\"img_dir\"),\n            image_transform=self.image_transform,\n            text_transform=self.text_transform,\n            # limit training samples only\n            dev_limit=(\n                self.hparams.get(\"dev_limit\", None) \n                if \"train\" in str(dataset_key) else None\n            ),\n            balance=True if \"train\" in str(dataset_key) else False,\n        )\n    \n    def _build_model(self):\n        # configure roberta model\n        #configuration = AutoConfig.from_pretrained(\n            # dropout\n            #hidden_dropout_prob=self.hparams.get(\"bert_hidden_dropout_p\", 0.05),\n            #attention_probs_dropout_prob=self.hparams.get(\"bert_attn_dropout_p\", 0.05),\n            \n        #) \n        # load pretrained roberta\n        language_module = gettextmodel()\n        \n        # we want to fine-tune the language module\n        for param in language_module.parameters():\n            param.requires_grad = True\n\n        #the out of language_module will be the input of FC layer\n        language_module.pooler = torch.nn.Linear(\n            in_features=768, # bert hidden size\n            out_features=self.language_feature_dim\n        )\n        \n        vision_module = torchvision.models.densenet201(pretrained=True)\n        \n        vision_module.classifier = torch.nn.Linear(\n                in_features=1920,\n                out_features=self.vision_feature_dim\n        )\n\n\n        # load pretrained resnet152\n        #vision_module = torchvision.models.resnet152(\n        #    pretrained=True\n        #)\n        #vision_module.fc = torch.nn.Linear(\n        #        in_features=2048,\n        #        out_features=self.vision_feature_dim\n        #)\n\n        return LanguageAndVisionConcat(\n            num_classes=self.hparams.get(\"num_classes\", 2),\n            loss_fn=torch.nn.CrossEntropyLoss(),\n            language_module=language_module, # the output of language_module is a tuple where the first value is the last_hidden_state\n            post_language_module=language_module.pooler,\n            vision_module=vision_module,\n            language_feature_dim=self.language_feature_dim,\n            vision_feature_dim=self.vision_feature_dim,\n            fusion_mid_size=self.hparams.get(\"fusion_mid_size\", 1024),\n            fusion_output_size=self.hparams.get(\n                \"fusion_output_size\", 512\n            ),\n            dropout_fusion_in_p=self.hparams.get(\"dropout_fusion_in_p\"),\n            dropout_fusion_mid_p=self.hparams.get(\"dropout_fusion_mid_p\"),\n            dropout_fusion_out_p=self.hparams.get(\"dropout_fusion_out_p\"),\n            dropout_vision_feature_p = self.hparams.get(\"dropout_vision_feature_p\"),\n            dropout_text_feature_p = self.hparams.get(\"dropout_text_feature_p\"),\n        )\n    \n    def _get_trainer_params(self):\n        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n            filepath=self.output_path,\n            monitor=self.hparams.get(\n                \"checkpoint_monitor\", \"val_loss\"\n            ),\n            mode=self.hparams.get(\n                \"checkpoint_monitor_mode\", \"min\"\n            ),\n            verbose=self.hparams.get(\"verbose\", True)\n        )\n\n        early_stop_callback = pl.callbacks.EarlyStopping(\n            monitor=self.hparams.get(\n                \"early_stop_monitor\", \"val_loss\"\n            ),\n            min_delta=self.hparams.get(\n                \"early_stop_min_delta\", 0.001\n            ),\n            patience=self.hparams.get(\n                \"early_stop_patience\", 3\n            ),\n            verbose=self.hparams.get(\"verbose\", True),\n        )\n\n        trainer_params = {\n            \"checkpoint_callback\": checkpoint_callback,\n            \"early_stop_callback\": early_stop_callback,\n            \"default_root_dir\": self.output_path,\n            \"accumulate_grad_batches\": self.hparams.get(\n                \"accumulate_grad_batches\", 1\n            ),\n            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n            \"gradient_clip_val\": self.hparams.get(\n                \"gradient_clip_value\", 1\n            ),\n        }\n        return trainer_params\n            \n    @torch.no_grad()\n    def make_submission_frame(self, test_path):\n        test_dataset = self._build_dataset(test_path)\n        submission_frame = pd.DataFrame(\n            index=test_dataset.samples_frame.id,\n            columns=[\"proba\", \"label\"]\n        )\n        test_dataloader = torch.utils.data.DataLoader(\n            test_dataset, \n            shuffle=False, \n            batch_size=self.hparams.get(\"batch_size\", 8), \n            num_workers=self.hparams.get(\"num_workers\", 16))\n        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n            preds, _ = self.model.eval()(\n                batch[\"text\"], batch[\"image\"]\n            )\n            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n        submission_frame.proba = submission_frame.proba.astype(float)\n        submission_frame.label = submission_frame.label.astype(int)\n        return submission_frame\n    \n    \n\"\"\"\nthe architecture of the model:\n    # 1. text_transform: FastText num of parameters: 0 because we use the pretrained model without fine-tuning\n    2. image_transform: ResNet152 num of parameters: 58,279,234\n    3. language_module: Linear(in_features=300, out_features=512, bias=True) num of parameters: 153,600\n    4. vision_module: Linear(in_features=2048, out_features=512, bias=True) num of parameters: 1,049,088\n    5. fusion: Linear(in_features=1024, out_features=512, bias=True) num of parameters: 524,800\n    6. output: Linear(in_features=512, out_features=2, bias=True) num of parameters: 1,026\n    total num of parameters: 59,007,748\n\"\"\"\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.982587Z","iopub.status.idle":"2023-06-12T22:39:21.983331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hparams = {\n    \n    # Required hparams\n    \"train_path\": train_path,\n    \"dev_path\": dev_path,\n    \"img_dir\": data_dir,\n    \n    # Optional hparams\n    #\"embedding_dim\": 150,\n    \"language_feature_dim\": 2048,\n    \"vision_feature_dim\": 2048,\n    \"fusion_mid_size\": 2048, \n    \"fusion_output_size\": 1024,\n    \"output_path\": \"model-outputs\",\n    \"dev_limit\": None,\n    \"lr\": 5e-6,\n    \"max_epochs\": 2,\n    \"n_gpu\": 1, # torch.cuda.device_count(),\n    \"batch_size\": 12,\n    # allows us to \"simulate\" having larger batches \n    \"accumulate_grad_batches\": 1, \n    \"early_stop_patience\": 4,\n    \"bert_hidden_dropout_p\": 0.1,\n    \"bert_attn_dropout_p\": 0.1,\n    \"dropout_fusion_in_p\": 0.05,\n    \"dropout_fusion_mid_p\": 0.05,\n    \"dropout_fusion_out_p\": 0.05,\n    \"dropout_vision_feature_p\": 0.05,\n    \"dropout_text_feature_p\": 0.05\n}\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.984892Z","iopub.status.idle":"2023-06-12T22:39:21.985668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model = HatefulMemesModel(hparams=hparams)\nhateful_memes_model.fit()\n\"\"\"\nsubmission = hateful_memes_model.make_submission_frame(\n    dev_path\n)\n\n# Assuming you have the submission frame with \"proba\" and \"label\" columns\nproba = torch.tensor(submission['proba'].values)\nlabel = torch.tensor(val['label'].values)\n\n# Calculate accuracy\npredictions = proba.round().long()\naccuracy = accuracy_score(label, predictions)\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.987163Z","iopub.status.idle":"2023-06-12T22:39:21.988001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeckpt():\n    directory = \"/kaggle/working/model-outputs/\"\n    files=os.listdir(directory)\n\n    for item in files:\n        if item.endswith(\".ckpt\"):\n            os.remove( os.path.join( directory, item ) )","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.995354Z","iopub.status.idle":"2023-06-12T22:39:21.996269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load optuna study (joblib file) if it exists\nif os.path.exists(\"/kaggle/input/optuna-study/xgb_optuna_study_batch.pkl\"):\n    # to load it:\n    study = joblib.load(f\"/kaggle/input/optuna-study/xgb_optuna_study_batch.pkl\")\nelse:\n    # define optuna params\n    study = optuna.create_study(direction=\"minimize\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.997646Z","iopub.status.idle":"2023-06-12T22:39:21.998534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new optuna study\nstudy = optuna.create_study(direction=\"minimize\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:21.999859Z","iopub.status.idle":"2023-06-12T22:39:22.000741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use optuna to tune hyperparameters\n# define objective function\ndef objective(trial):\n    # sample hpsearch params\n    hparams[\"lr\"] = trial.suggest_float(\"lr\",2e-7, 1e-5, log=True)\n    # hparams[\"dropout_in_p\"] = trial.suggest_float(\"dropout_in_p\", 0, 0.5, step=0.01)\n    # hparams[\"dropout_mid_p\"] = trial.suggest_float(\"dropout_mid_p\", 0, 0.5, step=0.01)\n    # hparams[\"dropout_out_p\"] = trial.suggest_float(\"dropout_out_p\", 0, 0.5, step=0.01)\n    # hparams[\"bert_hidden_dropout_p\"] = trial.suggest_float(\"bert_hidden_dropout_p\", 0, 0.5, step=0.01)\n    # hparams[\"bert_attn_dropout_p\"] = trial.suggest_float(\"bert_attn_dropout_p\", 0, 0.5, step=0.01)\n    #hparams[\"accumulate_grad_batches\"] = trial.suggest_categorical(\n    #    \"accumulate_grad_batches\", [1, 2, 4, 8, 16, 32]\n    #)\n    #hparams[\"batch_size\"] = trial.suggest_categorical(\n    #    \"batch_size\", [4, 8, 16, 32, 64]\n    #)\n    #hparams[\"language_feature_dim\"] = trial.suggest_int(\n    #    \"language_feature_dim\", 100, 500, step=10\n    #)\n    #hparams[\"vision_feature_dim\"] = trial.suggest_int(\n    #    \"vision_feature_dim\", 100, 500, step=10\n    #)\n    \n    #hparams[\"fusion_mid_size\"] = trial.suggest_int(\n    #    \"fusion_mid_size\", 10, 3000, step=10\n    #)\n    #hparams[\"fusion_output_size\"] = trial.suggest_int(\n    #    \"fusion_output_size\", 10, 3000, step=10\n    #)\n    print(\"lr = \", hparams[\"lr\"])\n    \n    \n    # train model\n    hateful_memes_model = HatefulMemesModel(hparams=hparams)\n    hateful_memes_model.fit()\n    \n    removeckpt()\n    \n    \n    \n    # report results back to optuna\n    return hateful_memes_model.trainer.callback_metrics[\"val_loss\"].item()\n\n\n\n\n# optimize model\nstudy.optimize(objective, n_trials=10, timeout=3600, show_progress_bar=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.002124Z","iopub.status.idle":"2023-06-12T22:39:22.003004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save study\njoblib.dump(study, f\"/kaggle/working/xgb_optuna_study_batch.pkl\")   \n\n# to load it:\njl = joblib.load(f\"/kaggle/working/xgb_optuna_study_batch.pkl\")\n\nprint(jl.best_trial.params)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.004339Z","iopub.status.idle":"2023-06-12T22:39:22.005203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"removeckpt()\n\n\nprint(hparams)\nhparams.update(study.best_trial.params)\nprint(\"\\n\\n\")\nprint(hparams)\nhparams[\"dev_limit\"] = None\nhparams[\"max_epochs\"] = 100\nhparams[\"early_stop_patience\"] = 6\n# train model\nhateful_memes_model = HatefulMemesModel(hparams=hparams)\nhateful_memes_model.fit()","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.006532Z","iopub.status.idle":"2023-06-12T22:39:22.007395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we should only have saved the best checkpoint\ncheckpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\n#assert len(checkpoints) == 1\n\ncheckpoints","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.008733Z","iopub.status.idle":"2023-06-12T22:39:22.009611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hateful_memes_model = HatefulMemesModel.load_from_checkpoint(\n    checkpoints[0].as_posix()\n)\nsubmission = hateful_memes_model.make_submission_frame(\n    test_path\n)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.010914Z","iopub.status.idle":"2023-06-12T22:39:22.011804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = hateful_memes_model.make_submission_frame(test_path)\nsubmission.groupby(\"label\").proba.mean()\nsubmission.label.value_counts()\nsubmission.to_csv((\"model-outputs/submission.csv\"), index=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.013169Z","iopub.status.idle":"2023-06-12T22:39:22.014061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\n\n# Assuming you have the submission frame with \"proba\" and \"label\" columns\nproba = torch.tensor(submission['proba'].values)\nlabel = torch.tensor(test['label'].values)\n\n# Calculate AUC-ROC score\nauc_roc = roc_auc_score(label, proba)\nprint(f\"AUC-ROC Score: {auc_roc}\")\n\n# Calculate accuracy\npredictions = proba.round().long()\naccuracy = accuracy_score(label, predictions)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-12T22:39:22.015465Z","iopub.status.idle":"2023-06-12T22:39:22.016348Z"},"trusted":true},"execution_count":null,"outputs":[]}]}